{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# TensorFlow for Deep Learning - Training Deep Neural Networks"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. The Vanishing/Exploding Gradients"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Unfortunately, gradients often get smaller and smaller as the backprop algorithm progresses down to the lower layers. As a result, the Gradient Descent update leaves the lower layer's connection weights virtually unchanged, and training never converges to a good solution. We call this the **vanishing gradients** problem. In some cases, the opposite can happen: the gradients can grow bigger and bigger until layers get insanely large weight updates and the algorithm diverges. This is the **exploding gradients** problem, which surfaces in recurrent neural networks. More generally, deep neural networks suffer from unstable gradients; different layers may learn at widely different speeds.\n",
    "\n",
    "Xavier Glorot and Yoshua Bengio in 2010 paper) showed that the combination of the popular _logistic sigmoid_ activation function and the weight initialization technique of a _normal distribution_ with a mean of 0 and a standard deviation of 1 results in variance of the outputs of each layer being much greater than the variance of its inputs. Going forward in the network, the variance keeps increasing after each layer until the activation function saturates at the top layers. This saturation is actually made worse by the fact that the logistic function has a mean of 0.5, not 0 (the hyperbolic tangent function has a mean of 0 and behaves slightly better than the logistic function in deep networks). Looking at the logistic activation function, you can see that when inputs become large (negative or positive), the function saturates at 0 or 1, with a derivative extremely close to 0. Thus, when backpropagation kicks in it has virtually no gradient to propagate back through the network; and what little gradient exists keeps getting diluted as backpropagation progresses down through the top layers, so there is really nothing left for the lower layers."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1a. Glorot and He Initialization"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The signal should flow properly in both directions: in the forward direction when making predictions, and in the reverse direction when backpropagating gradients. We don't want the signal to die out, nor do we want it to explode and saturate. For the signal to flow properly, the authors argue that we need the variance of the outputs of each layer to be equal to the variance of its inputs, and we need the gradients to have equal variance before and after flowing through a layer in the reverse direction.  It is actually not possible to guarantee both unless the layer has an equal number of inputs and neurons (these numbers are called the fan-in and fan-out of the layer), but Glorot and Bengio proposed a good compromise that has proven to work very well in practice: the connection weights of each layer must be initialized randomly: \n",
    "\n",
    "**Glorot initialization (when using the logistic activation function)**\n",
    "- Normal distribution with mean 0 and variance $ \\sigma^2 = {1}/{fan_{avg}} $, or\n",
    "- Uniform distribution between - r and + r, with $ r = \\sqrt{3/fan_{avg}}$\n",
    "\n",
    "where, $fan_{avg} = (fan_{in}+fan_{out})/2$, $fan_{in} = $ no. of inputs, $fan_{out} = $ no. of outputs\n",
    "\n",
    "Using Glorot initialization can speed up training considerably, and it is one of the tricks that led to the success of Deep Learning.\n",
    "\n",
    "<img src=\"images/TF_init.png\" style=\"float:center;\" width=\"350\"/>\n",
    "\n",
    "---\n",
    "When creating a layer, you can change this to He initialization by setting:\n",
    "\n",
    "```python\n",
    "keras.layers.Dense(10, activation=\"relu\", kernel_initializer=\"he_uniform\")\n",
    "keras.layers.Dense(10, activation=\"relu\", kernel_initializer=\"he_normal\")\n",
    "```\n",
    "\n",
    "He initialization with a uniform distribution but based on $fan_{avg}$ rather than $fan_{in}$:\n",
    "\n",
    "```python\n",
    "he_avg_init = keras.initializers.VarianceScaling(scale=2., mode='fan_avg', \n",
    "                                                 distribution='uniform')\n",
    "keras.layers.Dense(10, activation=\"sigmoid\", kernel_initializer=he_avg_init)\n",
    "```\n",
    "---\n",
    "**_Keras default: Glorot with a uniform distribution_**."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1b. Nonsaturating Activation Functions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "One of the insights in the 2010 paper by Glorot and Bengio was that the problems with unstable gradients were in part due to a poor choice of activation function. Until then most people had assumed that if Mother Nature had chosen to use roughly sigmoid activation functions in biological neurons, they must be an excellent choice. But it turns out that other activation functions behave much better in deep neural networks -- in particular, the **ReLU** activation function, _mostly because it does not saturate for positive values and because it is fast to compute_.\n",
    "\n",
    "**NOTE** - Unfortunately, the ReLU activation function is not perfect. It suffers from a problem known as the _dying ReLUs_: during training, some neurons (for any input) effectively **die**, meaning they stop outputting anything other than 0. In some cases, you may find that half of your network's neurons are dead, especially if you used a large learning rate. A neuron dies when its weights get tweaked in such a way that the weighted sum of its inputs are negative for all instances in the training set. When this happens, it just keeps outputting zeros, and Gradient Descent does not affect it anymore because the gradient of the ReLU function is zero when its input is negative.\n",
    "\n",
    "**SOLN** - To solve this problem, you may want to use a variant of the ReLU function, such as the leaky ReLU. This function is defined as $LeakyReLU_{\\alpha}(z) = max(\\alpha z, z)$. The hyperparameter $\\alpha$ defines how much the function **leaks**: it is the slope of the  function for $z < 0$ and is typically set to 0.01. This small slope ensures that leaky ReLUs never die; they can go into a long coma, but they have a chance to eventually wake up.\n",
    "\n",
    "- _randomized leaky ReLU_ (RReLU), where $\\alpha$ is picked randomly in a given range during training and is fixed to an average value during testing.\n",
    "- _parametric leaky ReLU_ (PReLU), where $\\alpha$ is authorized to be learned during training (instead of being a hyperparameter, it becomes a parameter that can be modified by  backpropagation like any other parameter)\n",
    "- _exponential linear unit_ (ELU) outperformed all the ReLU variants: training time was reduced, and the neural network performed better on the test set\n",
    "    $$ ELU_{\\alpha}(z) =  \\alpha(exp(z)-1) \\quad if \\ z<0 $$\n",
    "    $$ ELU_{\\alpha}(z) =  z \\quad if \\ z\\ge 0 $$\n",
    "\n",
    "    - It takes on negative values when $z < 0$, which allows the unit to have an average output closer to 0 and helps alleviate the vanishing gradients problem. The hyperparameter $\\alpha$ defines the value that the ELU function approaches when z is a large negative number. It is usually set to 1, but you can tweak it like any other hyperparameter. It has a nonzero gradient for $z < 0$, which avoids the dead neurons problem. If $\\alpha$ is equal to 1 then the function is smooth  everywhere, including around $z < 0$, which helps speed up Gradient Descent since it does not bounce as much to the left and right of $z<0$.\n",
    "\n",
    "    - The main drawback of the ELU activation function is that it is slower to compute than the ReLU function and its variants (due to the use of the exponential function). Its faster convergence rate during training  compensates for that slow computation, but still, at test time an ELU network will be slower than a ReLU network.\n",
    "    \n",
    "- _scaled ELU_ (SELU) activation function: it is a scaled variant of the ELU activation function. If you build a neural network composed exclusively of a stack of dense layers, and if all hidden layers use the SELU activation function, then the network will self-normalize: the output of each layer will tend to preserve a mean of 0 and standard deviation of 1 during training, which solves the vanishing/exploding gradients problem. As a result, the SELU activation function often significantly outperforms other activation functions for such neural nets (especially deep ones). There are, however, a few conditions for self-normalization to happen:\n",
    "    - The input features must be standardized\n",
    "    - Every hidden layer's weights must be initialized with LeCun normal initialization. In Keras, this means setting ```kernel_initializer=\"lecun_normal\"```\n",
    "    - The network??s architecture must be sequential\n",
    "    - The paper only guarantees self-normalization if all layers are dense, but some researchers have noted that the SELU activation function can improve performance in convolutional neural nets as well\n",
    "\n",
    "**TIPS**\n",
    "- Although your mileage will vary, in general SELU > ELU > leaky ReLU\n",
    "(and its variants) > ReLU > tanh > logistic.\n",
    "- If the network??s architecture prevents it from\n",
    "self-normalizing, then ELU may perform better than SELU (since SELU is not smooth at z = 0).\n",
    "- If you care a lot about runtime latency, then you may prefer leaky ReLU.\n",
    "- If you don't want to tweak yet another hyperparameter, you may use the default $\\alpha$ values used by Keras (e.g., 0.3 for leaky ReLU).\n",
    "- If you have spare time and computing power, you can use cross-validation to evaluate other activation functions, such as RReLU if your network is overfitting or PReLU if you have a huge training set.\n",
    "- That said, because ReLU is the most used activation function (by far), many libraries and hardware accelerators provide ReLU-specific  ptimizations; therefore, if speed is your priority, ReLU might still be the best choice.\n",
    "\n",
    "---\n",
    "To use the leaky ReLU activation function, create a LeakyReLU layer and\n",
    "add it to your model just after the layer you want to apply it to:\n",
    "\n",
    "```python\n",
    "model = keras.models.Sequential([ \n",
    "    [...] \n",
    "    keras.layers.Dense(10, kernel_initializer=\"he_normal\"), \n",
    "    keras.layers.LeakyReLU(alpha=0.2), \n",
    "    [...]\n",
    "])\n",
    "```\n",
    "\n",
    "For PReLU, replace LeakyRelu(alpha=0.2) with PReLU().\n",
    "\n",
    "For SELU activation, set activation=\"selu\" and\n",
    "kernel_initializer=\"lecun_normal\" when creating a layer:\n",
    "```python\n",
    "layer = keras.layers.Dense(10, activation=\"selu\", \n",
    "                           kernel_initializer=\"lecun_normal\")\n",
    "```\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1c. Batch Normalization"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Although using He initialization along with ELU (or any variant of ReLU) can significantly reduce the danger of the vanishing/exploding gradients problems at the _beginning_ of training, it doesn't guarantee that they won't\n",
    "come back during training.\n",
    "\n",
    "In a 2015 paper,  Sergey Ioffe and Christian Szegedy proposed a technique\n",
    "called **Batch Normalization** (BN) that addresses these problems. The\n",
    "technique consists of adding an operation in the model just before or after\n",
    "the activation function of each hidden layer. This operation simply zero-\n",
    "centers and normalizes each input, then scales and shifts the result using\n",
    "two new parameter vectors per layer: one for scaling, the other for  shifting. In other words, the operation lets the model learn the optimal scale and mean of each of the layer's inputs. \n",
    "\n",
    "**Train**\n",
    "\n",
    "In many cases, if you add a BN layer as the very first layer of your neural network, you do not need to standardize your training set (e.g., using a StandardScaler); the BN layer will do it for you (well, approximately, since it only looks at one batch at a time, and it can also rescale and shift each input feature). In order to zero-center and normalize the inputs, the algorithm needs to estimate each input's mean and standard deviation. It does so by evaluating the mean and standard deviation of the input over the current mini-batch (hence the name \"Batch Normalization\"). \n",
    "\n",
    "Standardization + Scaling + Shifting:\n",
    "\n",
    "<img src=\"images/TF_BN1.png\" width=\"300\"/>\n",
    "<img src=\"images/TF_BN2.png\" width=\"300\"/>\n",
    "\n",
    "**Test**\n",
    "\n",
    "So during training, BN standardizes its inputs, then rescales and offsets\n",
    "them. Good! What about at test time? We may need to make predictions for individual instances rather than for batches of instances: in this case, we will have no way to compute each input's mean and standard deviation. Moreover, even if we do have a batch of instances, it may be too small, or the instances may not be independent and identically distributed, so computing statistics over the batch instances would be unreliable.\n",
    "\n",
    "Most implementations of Batch Normalization estimate these final statistics\n",
    "during training by using a moving average of the layer's input means and\n",
    "standard deviations. This is what Keras does automatically when you use the BatchNormalization layer. To sum up, four parameter vectors are learned in each batch-normalized layer: $\\bold{\\gamma}$ (the output scale vector) and $\\bold{\\beta}$ (the output offset vector) are learned through regular backpropagation, and $\\bold{\\mu}$ (the final input mean vector) and $\\bold{\\sigma}$ (the final input standard deviation vector) are estimated using an exponential moving average. Note that $\\bold{\\mu}$ and $\\bold{\\sigma}$ are estimated during training, but they are used only after training (to replace the batch input means and standard deviations).\n",
    "\n",
    "PROS:\n",
    "\n",
    "- The vanishing gradients problem is strongly reduced, to the point that they could use saturating activation functions such as the tanh and even the logistic activation function. \n",
    "- The networks were are much less sensitive to the weight initialization. The authors were able to use much larger learning rates.\n",
    "- BN acts like a regularizer, reducing the need for other regularization techniques such as dropout.\n",
    "\n",
    "CONS(ish):\n",
    "\n",
    "- BN does add some complexity to the model\n",
    "- Training is rather slow, because each epoch takes much more time when you use Batch Normalization. This is usually counterbalanced by the fact that convergence is much faster with BN, so it will take fewer epochs to reach  the same performance. All in all, wall time will usually be shorter (this is the time measured by the clock on your wall).\n",
    "\n",
    "AVOIDING PENALTY:\n",
    "\n",
    "There is a runtime penalty: the neural network makes slower predictions due to the extra computations required at each layer. Fortunately, it's often possible to fuse the BN layer with the previous layer, after training, thereby avoiding the runtime penalty. This is done by updating the previous layer's weights and biases so that it directly produces outputs of the appropriate scale and offset: \n",
    "\n",
    "<img src=\"images/TF_BN3.png\" width=\"500\"/>\n",
    "\n",
    "---\n",
    "Implementing Batch Normalization with Keras: "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import tensorflow.keras as keras\n",
    "\n",
    "model = keras.models.Sequential([ \n",
    "    keras.layers.Flatten(input_shape=[28, 28]), \n",
    "    keras.layers.BatchNormalization(),\n",
    "\n",
    "    keras.layers.Dense(300,activation=\"elu\",kernel_initializer=\"he_normal\"), \n",
    "    keras.layers.BatchNormalization(), \n",
    "\n",
    "    keras.layers.Dense(100,activation=\"elu\",kernel_initializer=\"he_normal\"), \n",
    "    keras.layers.BatchNormalization(),\n",
    "\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 784)               3136      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 271,346\n",
      "Trainable params: 268,978\n",
      "Non-trainable params: 2,368\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "[(var.name, var.trainable) for var in model.layers[1].variables]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('batch_normalization/gamma:0', True),\n",
       " ('batch_normalization/beta:0', True),\n",
       " ('batch_normalization/moving_mean:0', False),\n",
       " ('batch_normalization/moving_variance:0', False)]"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Each BN layer adds four parameters per input: $\\gamma, \\beta, \\mu$ and $\\sigma$ (for example, the first BN layer adds 3,136 parameters, which is 4 x 784). The last two parameters, $\\mu$ and $\\sigma$, are the moving averages; they are not affected by backpropagation, so Keras calls them **non-trainable** params.\n",
    "\n",
    "In this tiny example with just two hidden layers, it's unlikely that BN will have a very positive impact; but for deeper networks it can make a tremendous difference. \n",
    "\n",
    "To add the BN layers before the activation functions, you must\n",
    "remove the activation function from the hidden layers and add them as\n",
    "separate layers after the BN layers. Moreover, since a Batch Normalization\n",
    "layer includes one offset parameter per input, you can remove the bias term\n",
    "from the previous layer (just pass ```use_bias=False``` when creating it):"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "model = keras.models.Sequential([ \n",
    "    keras.layers.Flatten(input_shape=[28, 28]), \n",
    "    keras.layers.BatchNormalization(), \n",
    "\n",
    "    keras.layers.Dense(300, kernel_initializer=\"he_normal\", use_bias=False), \n",
    "    keras.layers.BatchNormalization(), \n",
    "    keras.layers.Activation(\"elu\"), \n",
    "\n",
    "    keras.layers.Dense(100, kernel_initializer=\"he_normal\", use_bias=False), \n",
    "    keras.layers.BatchNormalization(), \n",
    "    keras.layers.Activation(\"elu\"),\n",
    "    \n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 784)               3136      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 300)               235200    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 100)               30000     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 270,946\n",
      "Trainable params: 268,578\n",
      "Non-trainable params: 2,368\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The BatchNormalization class has quite a few hyperparameters you can\n",
    "tweak. The defaults will usually be fine, except for ```momentum``` and ```axis```.\n",
    "\n",
    "```python\n",
    "momentum\n",
    "```\n",
    "occasionally need to tweak the ```momentum```. This hyperparameter is used by the BatchNormalization layer when it updates the exponential moving\n",
    "averages; given a new value v (i.e., a new vector of input means or standard\n",
    "deviations computed over the current batch), the layer updates the running\n",
    "average $\\hat{v}$ using the following equation:\n",
    "\n",
    "$$ v \\leftarrow v \\times momentum + v \\times (1-momentum) $$\n",
    "\n",
    "A good momentum value is typically close to 1; for example, 0.9, 0.99, or\n",
    "0.999 (you want more 9s for larger datasets and smaller mini-batches).\n",
    "\n",
    "```python\n",
    "axis\n",
    "```\n",
    "it determines which axis should be normalized. It defaults to -1, meaning that by default it will normalize the last axis (using the means and  standard deviations computed across the other axes). When the input batch is 2D (i.e., the batch shape is [_batch size,features_]), this means that each input feature will be normalized based on the mean and standard deviation computed across all the instances in the batch.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Notice that the BN layer does not perform the same computation during\n",
    "training and after training: it uses batch statistics during training and the \"final\" statistics after training (i.e., the final values of the moving averages). Let's take a peek at the source code of this class to see how this is handled:\n",
    "\n",
    "```python\n",
    "class BatchNormalization(keras.layers.Layer): \n",
    "    [...] \n",
    "    def call(self, inputs, training=None): \n",
    "        [...]\n",
    "```\n",
    "\n",
    "The ```call()``` method is the one that performs the computations; as you can\n",
    "see, it has an extra ```training``` argument, which is set to None by default, but the ```fit()``` method sets to it to 1 during training. If you ever need to write a custom layer, and it must behave differently during training and testing, add a training argument to the ```call()``` method and use this argument in the method to decide what to compute.\n",
    "\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1d. Gradient Clipping"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Another popular technique to mitigate the exploding gradients problem is to\n",
    "clip the gradients during backpropagation so that they never exceed some\n",
    "threshold. This is called **Gradient Clipping**. This technique is most often\n",
    "used in recurrent neural networks, as Batch Normalization is tricky to use in\n",
    "RNNs. For other types of networks, BN is usually sufficient.\n",
    "\n",
    "---\n",
    "In Keras, implementing Gradient Clipping is just a matter of setting the\n",
    "```clipvalue``` or ```clipnorm``` argument when creating an optimizer:\n",
    "\n",
    "```python\n",
    "optimizer = keras.optimizers.SGD(clipvalue=1.0)\n",
    "model.compile(loss=\"mse\", optimizer=optimizer)\n",
    "```\n",
    "---\n",
    "\n",
    "This optimizer will clip every component of the gradient vector to a value\n",
    "between -1.0 and 1.0. This means that all the partial derivatives of the loss\n",
    "(with regard to each and every trainable parameter) will be clipped between\n",
    "-1.0 and 1.0. The threshold is a hyperparameter you can tune. Note that it\n",
    "may change the orientation of the gradient vector. For instance, if the\n",
    "original gradient vector is [0.9, 100.0], it points mostly in the direction of the second axis; but once you clip it by value, you get [0.9, 1.0], which\n",
    "points roughly in the diagonal between the two axes. In practice, this\n",
    "approach works well. If you want to ensure that Gradient Clipping does not\n",
    "change the direction of the gradient vector, you should clip by norm by\n",
    "setting ```clipnorm``` instead of ```clipvalue```. This will clip the whole gradient if its $l_2$ norm is greater than the threshold you picked. For example, if you set ```clipnorm=1.0```, then the vector [0.9, 100.0] will be clipped to [0.00899964, 0.9999595], preserving its orientation but almost eliminating the first component.\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Reusing Pretrained Layers"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "It will not only speed up training considerably, but also require significantly less training data. Suppose you have access to a DNN that was trained to classify pictures into 100 different categories, including animals, plants, vehicles, and everyday objects. You now want to train a DNN to classify specific types of vehicles. These tasks are very similar, even partly overlapping, so you should try to reuse parts of the first network. \n",
    "\n",
    "Different input size: \n",
    "\n",
    "If the inputs of the new task don't have the same size as the ones used in  the original task, we usually have to add a preprocessing step to resize them to the size expected by the original model. More generally, transfer learning will work best when the inputs have similar low-level features.\n",
    "\n",
    "Similar tasks:\n",
    "\n",
    "The more similar the tasks are, the more layers you want to reuse (starting with the lower layers). For very similar tasks, try keeping all the hidden layers and just replacing the output layer.\n",
    "\n",
    "Unfreezing hidden layer(s):\n",
    "\n",
    "Try freezing all the reused layers first (i.e., make their weights non-trainable so that Gradient Descent won't modify them), then train your model and see how it performs. Then try unfreezing one or two of the top hidden layers to let backpropagation tweak them and see if performance improves. The more training data you have, the more layers you can unfreeze. It is also useful to reduce the learning rate when you unfreeze reused layers: this will avoid wrecking their fine-tuned weights."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2a. Transfer Learning with Keras:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Suppose the Fashion MNIST dataset only contained eight classes - for example, all the classes except for sandal and shirt. Someone built and trained a Keras model on that set and got reasonably good performance (>90% accuracy). Let's call this model A. You now want to tackle a different task: you have images of sandals and shirts, and you want to train a binary  lassifier (positive=shirt, negative=sandal). Your dataset is quite small; you only have 200 labeled images. When you train a new model for this task (let's call it model B) with the same architecture as model A, it performs reasonably well (97.2% accuracy). But since it's a much easier task (there are just two classes), you were hoping for more. You realize that\n",
    "your task is quite similar to task A, so perhaps transfer learning can help?\n",
    "\n",
    "```python\n",
    "model_A = keras.models.load_model(\"my_model_A.h5\")\n",
    "model_B_on_A = keras.models.Sequential(model_A.layers[:-1])\n",
    "model_B_on_A.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "```\n",
    "\n",
    "Note that ```model_A``` and ```model_B_on_A``` now share some layers. When you train ```model_B_on_A```, it will also affect ```model_A```. If you want to avoid that, you need to _clone_ ```model_A``` before you reuse its layers. To do this, you clone model A's architecture with ```clone.model()```, then copy its weights (since ```clone_model()``` does not clone the weights):\n",
    "\n",
    "```python\n",
    "model_A_clone = keras.models.clone_model(model_A)\n",
    "model_A_clone.set_weights(model_A.get_weights())\n",
    "```\n",
    "\n",
    "Now you could train model_B_on_A for task B, but since the new output layer was initialized randomly it will make large errors (at least during the first few epochs), so there will be large error gradients that may wreck the\n",
    "reused weights. To avoid this, one approach is to freeze the reused layers\n",
    "during the first few epochs, giving the new layer some time to learn\n",
    "reasonable weights. To do this, set every layer's trainable attribute to\n",
    "False and compile the model:\n",
    "\n",
    "```python\n",
    "for layer in model_B_on_A.layers[:-1]: \n",
    "    layer.trainable = False \n",
    " \n",
    "model_B_on_A.compile(loss=\"binary_crossentropy\", optimizer=\"sgd\", \n",
    "                     metrics=[\"accuracy\"])\n",
    "```\n",
    "\n",
    "**Note**:You must always compile your model after you freeze or unfreeze layers.\n",
    "\n",
    "Now you can train the model for a few epochs, then unfreeze the reused\n",
    "layers (which requires compiling the model again) and continue training to\n",
    "fine-tune the reused layers for task B. After unfreezing the reused layers, it is usually a good idea to reduce the learning rate, once again to avoid\n",
    "damaging the reused weights:\n",
    "\n",
    "```python\n",
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=4, \n",
    "                           validation_data=(X_valid_B, y_valid_B)) \n",
    " \n",
    "for layer in model_B_on_A.layers[:-1]: \n",
    "    layer.trainable = True \n",
    " \n",
    "optimizer = keras.optimizers.SGD(lr=1e-4) # the default lr is 1e-2\n",
    "model_B_on_A.compile(loss=\"binary_crossentropy\", optimizer=optimizer, \n",
    "                     metrics=[\"accuracy\"])\n",
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=16, \n",
    "                           validation_data=(X_valid_B, y_valid_B))\n",
    "```\n",
    "\n",
    "This model's test accuracy is 99.25%, which means that transfer learning reduced the error rate from 2.8% down to almost 0.7%! Are you convinced? You shouldn't be: I cheated! I tried many configurations until I found one that demonstrated a strong improvement. Why did I cheat? It turns out that transfer learning does not work very well with small dense networks, presumably because small networks learn few patterns, and dense networks learn very specific patterns, which are unlikely to be useful in other tasks. Transfer learning works best with deep convolutional neural networks, which tend to learn feature detectors that are much more general (especially in the lower layers)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2b. Unsupervised Pretraining -- pending"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2c. Pretraining on an Auxiliary Task -- pending"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Faster Optimizers"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9cdf6fd4ca346b1685b865c1c06435fd6596898e7539d13aac3a66e78bae3ff1"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.6 64-bit ('tfEnv_1': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}