{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow for Deep Learning - Training Deep Neural Networks\n",
    "\n",
    "Credits - [Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/)\n",
    "\n",
    "Good reads \n",
    "- [Yes you should understand backprop](https://karpathy.medium.com/yes-you-should-understand-backprop-e2f06eab496b#.vt3ax2kg9)\n",
    "- [Why Momentum Really Works](https://distill.pub/2017/momentum/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Vanishing/Exploding Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[<img src=\"images/VanGrad.png\" width=\"500\"/>](https://youtu.be/W_JJm_5syFw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, gradients often get smaller and smaller as the backprop algorithm progresses down to the lower layers. As a result, the Gradient Descent update leaves the lower layer's connection weights virtually unchanged, and training never converges to a good solution. We call this the **vanishing gradients** problem. In some cases, the opposite can happen: the gradients can grow bigger and bigger until layers get insanely large weight updates and the algorithm diverges. This is the **exploding gradients** problem, which surfaces in recurrent neural networks. More generally, deep neural networks suffer from unstable gradients; different layers may learn at widely different speeds.\n",
    "\n",
    "Xavier Glorot and Yoshua Bengio in 2010 paper) showed that the combination of the popular _logistic sigmoid_ activation function and the weight initialization technique of a _normal distribution_ with a mean of 0 and a standard deviation of 1 results in variance of the outputs of each layer being much greater than the variance of its inputs. Going forward in the network, the variance keeps increasing after each layer until the activation function saturates at the top layers. This saturation is actually made worse by the fact that the logistic function has a mean of 0.5, not 0 (the hyperbolic tangent function has a mean of 0 and behaves slightly better than the logistic function in deep networks). Looking at the logistic activation function, you can see that when inputs become large (negative or positive), the function saturates at 0 or 1, with a derivative extremely close to 0. Thus, when backpropagation kicks in it has virtually no gradient to propagate back through the network; and what little gradient exists keeps getting diluted as backpropagation progresses down through the top layers, so there is really nothing left for the lower layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1a. Glorot and He Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The signal should flow properly in both directions: in the forward direction when making predictions, and in the reverse direction when backpropagating gradients. We don't want the signal to die out, nor do we want it to explode and saturate. For the signal to flow properly, the authors argue that we need the variance of the outputs of each layer to be equal to the variance of its inputs, and we need the gradients to have equal variance before and after flowing through a layer in the reverse direction.  It is actually not possible to guarantee both unless the layer has an equal number of inputs and neurons (these numbers are called the fan-in and fan-out of the layer), but Glorot and Bengio proposed a good compromise that has proven to work very well in practice: the connection weights of each layer must be initialized randomly: \n",
    "\n",
    "**Glorot initialization (when using the logistic activation function)**\n",
    "- Normal distribution with mean 0 and variance $ \\sigma^2 = {1}/{fan_{avg}} $, or\n",
    "- Uniform distribution between - r and + r, with $ r = \\sqrt{3/fan_{avg}}$\n",
    "\n",
    "where, $fan_{avg} = (fan_{in}+fan_{out})/2$, $fan_{in} = $ no. of inputs, $fan_{out} = $ no. of outputs\n",
    "\n",
    "Using Glorot initialization can speed up training considerably, and it is one of the tricks that led to the success of Deep Learning.\n",
    "\n",
    "<img src=\"images/TF_init.png\" style=\"float:center;\" width=\"350\"/>\n",
    "\n",
    "---\n",
    "When creating a layer, you can change this to He initialization by setting:\n",
    "\n",
    "```python\n",
    "keras.layers.Dense(10, activation=\"relu\", kernel_initializer=\"he_uniform\")\n",
    "keras.layers.Dense(10, activation=\"relu\", kernel_initializer=\"he_normal\")\n",
    "```\n",
    "\n",
    "He initialization with a uniform distribution but based on $fan_{avg}$ rather than $fan_{in}$:\n",
    "\n",
    "```python\n",
    "he_avg_init = keras.initializers.VarianceScaling(scale=2., mode='fan_avg', \n",
    "                                                 distribution='uniform')\n",
    "keras.layers.Dense(10, activation=\"sigmoid\", kernel_initializer=he_avg_init)\n",
    "```\n",
    "---\n",
    "**_Keras default: Glorot with a uniform distribution_**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1b. Nonsaturating Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the insights in the 2010 paper by Glorot and Bengio was that the problems with unstable gradients were in part due to a poor choice of activation function. Until then most people had assumed that if Mother Nature had chosen to use roughly sigmoid activation functions in biological neurons, they must be an excellent choice. But it turns out that other activation functions behave much better in deep neural networks -- in particular, the **ReLU** activation function, _mostly because it does not saturate for positive values and because it is fast to compute_.\n",
    "\n",
    "**PROBLEM** - Unfortunately, the ReLU activation function is not perfect. It suffers from a problem known as the _dying ReLUs_: during training, some neurons (for any input) effectively **die**, meaning they stop outputting anything other than 0. In some cases, you may find that half of your network's neurons are dead, especially if you used a large learning rate. A neuron dies when its weights get tweaked in such a way that the weighted sum of its inputs are negative for all instances in the training set. When this happens, it just keeps outputting zeros, and Gradient Descent does not affect it anymore because the gradient of the ReLU function is zero when its input is negative.\n",
    "\n",
    "**SOLN** - To solve this problem, you may want to use a variant of the ReLU function, such as the leaky ReLU. This function is defined as $LeakyReLU_{\\alpha}(z) = max(\\alpha z, z)$. The hyperparameter $\\alpha$ defines how much the function **leaks**: it is the slope of the  function for $z < 0$ and is typically set to 0.01. This small slope ensures that leaky ReLUs never die; they can go into a long coma, but they have a chance to eventually wake up.\n",
    "\n",
    "- _randomized leaky ReLU_ (RReLU), where $\\alpha$ is picked randomly in a given range during training and is fixed to an average value during testing.\n",
    "- _parametric leaky ReLU_ (PReLU), where $\\alpha$ is authorized to be learned during training (instead of being a hyperparameter, it becomes a parameter that can be modified by  backpropagation like any other parameter)\n",
    "- _exponential linear unit_ (ELU) outperformed all the ReLU variants: training time was reduced, and the neural network performed better on the test set\n",
    "    $$ ELU_{\\alpha}(z) =  \\alpha(exp(z)-1) \\quad if \\ z<0 $$\n",
    "    $$ ELU_{\\alpha}(z) =  z \\quad if \\ z\\ge 0 $$\n",
    "\n",
    "    - It takes on negative values when $z < 0$, which allows the unit to have an average output closer to 0 and helps alleviate the vanishing gradients problem. The hyperparameter $\\alpha$ defines the value that the ELU function approaches when z is a large negative number. It is usually set to 1, but you can tweak it like any other hyperparameter. It has a nonzero gradient for $z < 0$, which avoids the dead neurons problem. If $\\alpha$ is equal to 1 then the function is smooth  everywhere, including around $z < 0$, which helps speed up Gradient Descent since it does not bounce as much to the left and right of $z<0$.\n",
    "\n",
    "    - The main drawback of the ELU activation function is that it is slower to compute than the ReLU function and its variants (due to the use of the exponential function). Its faster convergence rate during training  compensates for that slow computation, but still, at test time an ELU network will be slower than a ReLU network.\n",
    "    \n",
    "- _scaled ELU_ (SELU) activation function: it is a scaled variant of the ELU activation function. If you build a neural network composed exclusively of a stack of dense layers, and if all hidden layers use the SELU activation function, then the network will self-normalize: the output of each layer will tend to preserve a mean of 0 and standard deviation of 1 during training, which solves the vanishing/exploding gradients problem. As a result, the SELU activation function often significantly outperforms other activation functions for such neural nets (especially deep ones). There are, however, a few conditions for self-normalization to happen:\n",
    "    - The input features must be standardized\n",
    "    - Every hidden layer's weights must be initialized with LeCun normal initialization. In Keras, this means setting ```kernel_initializer=\"lecun_normal\"```\n",
    "    - The network's architecture must be sequential\n",
    "    - The paper only guarantees self-normalization if all layers are dense, but some researchers have noted that the SELU activation function can improve performance in convolutional neural nets as well\n",
    "\n",
    "**TIPS**\n",
    "- Although your mileage will vary, in general SELU > ELU > leaky ReLU\n",
    "(and its variants) > ReLU > tanh > logistic.\n",
    "- If the network's architecture prevents it from\n",
    "self-normalizing, then ELU may perform better than SELU (since SELU is not smooth at z = 0).\n",
    "- If you care a lot about runtime latency, then you may prefer leaky ReLU.\n",
    "- If you don't want to tweak yet another hyperparameter, you may use the default $\\alpha$ values used by Keras (e.g., 0.3 for leaky ReLU).\n",
    "- If you have spare time and computing power, you can use cross-validation to evaluate other activation functions, such as RReLU if your network is overfitting or PReLU if you have a huge training set.\n",
    "- That said, because ReLU is the most used activation function (by far), many libraries and hardware accelerators provide ReLU-specific  ptimizations; therefore, if speed is your priority, ReLU might still be the best choice.\n",
    "\n",
    "---\n",
    "To use the leaky ReLU activation function, create a LeakyReLU layer and\n",
    "add it to your model just after the layer you want to apply it to:\n",
    "\n",
    "```python\n",
    "model = keras.models.Sequential([ \n",
    "    [...] \n",
    "    keras.layers.Dense(10, kernel_initializer=\"he_normal\"), \n",
    "    keras.layers.LeakyReLU(alpha=0.2), \n",
    "    [...]\n",
    "])\n",
    "```\n",
    "\n",
    "For PReLU, replace LeakyRelu(alpha=0.2) with PReLU().\n",
    "\n",
    "For SELU activation, set activation=\"selu\" and\n",
    "kernel_initializer=\"lecun_normal\" when creating a layer:\n",
    "```python\n",
    "layer = keras.layers.Dense(10, activation=\"selu\", \n",
    "                           kernel_initializer=\"lecun_normal\")\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1c. Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although using He initialization along with ELU (or any variant of ReLU) can significantly reduce the danger of the vanishing/exploding gradients problems at the _beginning_ of training, it doesn't guarantee that they won't\n",
    "come back during training.\n",
    "\n",
    "In a 2015 paper,  Sergey Ioffe and Christian Szegedy proposed a technique\n",
    "called **Batch Normalization** (BN) that addresses these problems. The\n",
    "technique consists of adding an operation in the model just before or after\n",
    "the activation function of each hidden layer. This operation simply zero-\n",
    "centers and normalizes each input, then scales and shifts the result using\n",
    "two new parameter vectors per layer: one for scaling, the other for  shifting. In other words, the operation lets the model learn the optimal scale and mean of each of the layer's inputs. \n",
    "\n",
    "**Train**\n",
    "\n",
    "In many cases, if you add a BN layer as the very first layer of your neural network, you do not need to standardize your training set (e.g., using a StandardScaler); the BN layer will do it for you (well, approximately, since it only looks at one batch at a time, and it can also rescale and shift each input feature). In order to zero-center and normalize the inputs, the algorithm needs to estimate each input's mean and standard deviation. It does so by evaluating the mean and standard deviation of the input over the current mini-batch (hence the name \"Batch Normalization\"). \n",
    "\n",
    "Standardization + Scaling + Shifting:\n",
    "\n",
    "<img src=\"images/TF_BN1.png\" width=\"300\"/>\n",
    "<img src=\"images/TF_BN2.png\" width=\"300\"/>\n",
    "\n",
    "**Test**\n",
    "\n",
    "So during training, BN standardizes its inputs, then rescales and offsets\n",
    "them. Good! What about at test time? We may need to make predictions for individual instances rather than for batches of instances: in this case, we will have no way to compute each input's mean and standard deviation. Moreover, even if we do have a batch of instances, it may be too small, or the instances may not be independent and identically distributed, so computing statistics over the batch instances would be unreliable.\n",
    "\n",
    "Most implementations of Batch Normalization estimate these final statistics\n",
    "during training by using a moving average of the layer's input means and\n",
    "standard deviations. This is what Keras does automatically when you use the BatchNormalization layer. To sum up, four parameter vectors are learned in each batch-normalized layer: $\\bold{\\gamma}$ (the output scale vector) and $\\bold{\\beta}$ (the output offset vector) are learned through regular backpropagation, and $\\bold{\\mu}$ (the final input mean vector) and $\\bold{\\sigma}$ (the final input standard deviation vector) are estimated using an exponential moving average. Note that $\\bold{\\mu}$ and $\\bold{\\sigma}$ are estimated during training, but they are used only after training (to replace the batch input means and standard deviations).\n",
    "\n",
    "PROS:\n",
    "\n",
    "- The vanishing gradients problem is strongly reduced, to the point that they could use saturating activation functions such as the tanh and even the logistic activation function. \n",
    "- The networks were are much less sensitive to the weight initialization. The authors were able to use much larger learning rates.\n",
    "- BN acts like a regularizer, reducing the need for other regularization techniques such as dropout.\n",
    "\n",
    "CONS(ish):\n",
    "\n",
    "- BN does add some complexity to the model\n",
    "- Training is rather slow, because each epoch takes much more time when you use Batch Normalization. This is usually counterbalanced by the fact that convergence is much faster with BN, so it will take fewer epochs to reach  the same performance. All in all, wall time will usually be shorter (this is the time measured by the clock on your wall).\n",
    "\n",
    "AVOIDING PENALTY:\n",
    "\n",
    "There is a runtime penalty: the neural network makes slower predictions due to the extra computations required at each layer. Fortunately, it's often possible to fuse the BN layer with the previous layer, after training, thereby avoiding the runtime penalty. This is done by updating the previous layer's weights and biases so that it directly produces outputs of the appropriate scale and offset: \n",
    "\n",
    "<img src=\"images/TF_BN3.png\" width=\"500\"/>\n",
    "\n",
    "---\n",
    "Implementing Batch Normalization with Keras: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 784)               3136      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 271,346\n",
      "Trainable params: 268,978\n",
      "Non-trainable params: 2,368\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras as keras\n",
    "\n",
    "model = keras.models.Sequential([ \n",
    "    keras.layers.Flatten(input_shape=[28, 28]), \n",
    "    keras.layers.BatchNormalization(),\n",
    "\n",
    "    keras.layers.Dense(300,activation=\"elu\",kernel_initializer=\"he_normal\"), \n",
    "    keras.layers.BatchNormalization(), \n",
    "\n",
    "    keras.layers.Dense(100,activation=\"elu\",kernel_initializer=\"he_normal\"), \n",
    "    keras.layers.BatchNormalization(),\n",
    "\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('batch_normalization/gamma:0', True),\n",
       " ('batch_normalization/beta:0', True),\n",
       " ('batch_normalization/moving_mean:0', False),\n",
       " ('batch_normalization/moving_variance:0', False)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(var.name, var.trainable) for var in model.layers[1].variables]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each BN layer adds four parameters per input: $\\gamma, \\beta, \\mu$ and $\\sigma$ (for example, the first BN layer adds 3,136 parameters, which is 4 x 784). The last two parameters, $\\mu$ and $\\sigma$, are the moving averages; they are not affected by backpropagation, so Keras calls them **non-trainable** params.\n",
    "\n",
    "In this tiny example with just two hidden layers, it's unlikely that BN will have a very positive impact; but for deeper networks it can make a tremendous difference. \n",
    "\n",
    "To add the BN layers before the activation functions, you must\n",
    "remove the activation function from the hidden layers and add them as\n",
    "separate layers after the BN layers. Moreover, since a Batch Normalization\n",
    "layer includes one offset parameter per input, you can remove the bias term\n",
    "from the previous layer (just pass ```use_bias=False``` when creating it):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 784)               3136      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 300)               235200    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 100)               30000     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 270,946\n",
      "Trainable params: 268,578\n",
      "Non-trainable params: 2,368\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([ \n",
    "    keras.layers.Flatten(input_shape=[28, 28]), \n",
    "    keras.layers.BatchNormalization(), \n",
    "\n",
    "    keras.layers.Dense(300, kernel_initializer=\"he_normal\", use_bias=False), \n",
    "    keras.layers.BatchNormalization(), \n",
    "    keras.layers.Activation(\"elu\"), \n",
    "\n",
    "    keras.layers.Dense(100, kernel_initializer=\"he_normal\", use_bias=False), \n",
    "    keras.layers.BatchNormalization(), \n",
    "    keras.layers.Activation(\"elu\"),\n",
    "    \n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BatchNormalization class has quite a few hyperparameters you can\n",
    "tweak. The defaults will usually be fine, except for ```momentum``` and ```axis```.\n",
    "\n",
    "```python\n",
    "momentum\n",
    "```\n",
    "occasionally need to tweak the ```momentum```. This hyperparameter is used by the BatchNormalization layer when it updates the exponential moving\n",
    "averages; given a new value v (i.e., a new vector of input means or standard\n",
    "deviations computed over the current batch), the layer updates the running\n",
    "average $\\hat{v}$ using the following equation:\n",
    "\n",
    "$$ v \\leftarrow v \\times momentum + v \\times (1-momentum) $$\n",
    "\n",
    "A good momentum value is typically close to 1; for example, 0.9, 0.99, or\n",
    "0.999 (you want more 9s for larger datasets and smaller mini-batches).\n",
    "\n",
    "```python\n",
    "axis\n",
    "```\n",
    "it determines which axis should be normalized. It defaults to -1, meaning that by default it will normalize the last axis (using the means and  standard deviations computed across the other axes). When the input batch is 2D (i.e., the batch shape is [_batch size,features_]), this means that each input feature will be normalized based on the mean and standard deviation computed across all the instances in the batch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the BN layer does not perform the same computation during\n",
    "training and after training: it uses batch statistics during training and the \"final\" statistics after training (i.e., the final values of the moving averages). Let's take a peek at the source code of this class to see how this is handled:\n",
    "\n",
    "```python\n",
    "class BatchNormalization(keras.layers.Layer): \n",
    "    [...] \n",
    "    def call(self, inputs, training=None): \n",
    "        [...]\n",
    "```\n",
    "\n",
    "The ```call()``` method is the one that performs the computations; as you can\n",
    "see, it has an extra ```training``` argument, which is set to None by default, but the ```fit()``` method sets to it to 1 during training. If you ever need to write a custom layer, and it must behave differently during training and testing, add a training argument to the ```call()``` method and use this argument in the method to decide what to compute.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1d. Gradient Clipping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another popular technique to mitigate the exploding gradients problem is to\n",
    "clip the gradients during backpropagation so that they never exceed some\n",
    "threshold. This is called **Gradient Clipping**. This technique is most often\n",
    "used in recurrent neural networks, as Batch Normalization is tricky to use in\n",
    "RNNs. For other types of networks, BN is usually sufficient.\n",
    "\n",
    "---\n",
    "In Keras, implementing Gradient Clipping is just a matter of setting the\n",
    "```clipvalue``` or ```clipnorm``` argument when creating an optimizer:\n",
    "\n",
    "```python\n",
    "optimizer = keras.optimizers.SGD(clipvalue=1.0)\n",
    "model.compile(loss=\"mse\", optimizer=optimizer)\n",
    "```\n",
    "---\n",
    "\n",
    "This optimizer will clip every component of the gradient vector to a value\n",
    "between -1.0 and 1.0. This means that all the partial derivatives of the loss\n",
    "(with regard to each and every trainable parameter) will be clipped between\n",
    "-1.0 and 1.0. The threshold is a hyperparameter you can tune. Note that it\n",
    "may change the orientation of the gradient vector. For instance, if the\n",
    "original gradient vector is [0.9, 100.0], it points mostly in the direction of the second axis; but once you clip it by value, you get [0.9, 1.0], which\n",
    "points roughly in the diagonal between the two axes. In practice, this\n",
    "approach works well. If you want to ensure that Gradient Clipping does not\n",
    "change the direction of the gradient vector, you should clip by norm by\n",
    "setting ```clipnorm``` instead of ```clipvalue```. This will clip the whole gradient if its $l_2$ norm is greater than the threshold you picked. For example, if you set ```clipnorm=1.0```, then the vector [0.9, 100.0] will be clipped to [0.00899964, 0.9999595], preserving its orientation but almost eliminating the first component.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Reusing Pretrained Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will not only speed up training considerably, but also require significantly less training data. Suppose you have access to a DNN that was trained to classify pictures into 100 different categories, including animals, plants, vehicles, and everyday objects. You now want to train a DNN to classify specific types of vehicles. These tasks are very similar, even partly overlapping, so you should try to reuse parts of the first network. \n",
    "\n",
    "Different input size: \n",
    "\n",
    "If the inputs of the new task don't have the same size as the ones used in  the original task, we usually have to add a preprocessing step to resize them to the size expected by the original model. More generally, transfer learning will work best when the inputs have similar low-level features.\n",
    "\n",
    "Similar tasks:\n",
    "\n",
    "The more similar the tasks are, the more layers you want to reuse (starting with the lower layers). For very similar tasks, try keeping all the hidden layers and just replacing the output layer.\n",
    "\n",
    "Unfreezing hidden layer(s):\n",
    "\n",
    "Try freezing all the reused layers first (i.e., make their weights non-trainable so that Gradient Descent won't modify them), then train your model and see how it performs. Then try unfreezing one or two of the top hidden layers to let backpropagation tweak them and see if performance improves. The more training data you have, the more layers you can unfreeze. It is also useful to reduce the learning rate when you unfreeze reused layers: this will avoid wrecking their fine-tuned weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a. Transfer Learning with Keras:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose the Fashion MNIST dataset only contained eight classes - for example, all the classes except for sandal and shirt. Someone built and trained a Keras model on that set and got reasonably good performance (>90% accuracy). Let's call this model A. You now want to tackle a different task: you have images of sandals and shirts, and you want to train a binary  lassifier (positive=shirt, negative=sandal). Your dataset is quite small; you only have 200 labeled images. When you train a new model for this task (let's call it model B) with the same architecture as model A, it performs reasonably well (97.2% accuracy). But since it's a much easier task (there are just two classes), you were hoping for more. You realize that\n",
    "your task is quite similar to task A, so perhaps transfer learning can help?\n",
    "\n",
    "```python\n",
    "model_A = keras.models.load_model(\"my_model_A.h5\")\n",
    "model_B_on_A = keras.models.Sequential(model_A.layers[:-1])\n",
    "model_B_on_A.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "```\n",
    "\n",
    "Note that ```model_A``` and ```model_B_on_A``` now share some layers. When you train ```model_B_on_A```, it will also affect ```model_A```. If you want to avoid that, you need to _clone_ ```model_A``` before you reuse its layers. To do this, you clone model A's architecture with ```clone.model()```, then copy its weights (since ```clone_model()``` does not clone the weights):\n",
    "\n",
    "```python\n",
    "model_A_clone = keras.models.clone_model(model_A)\n",
    "model_A_clone.set_weights(model_A.get_weights())\n",
    "```\n",
    "\n",
    "Now you could train model_B_on_A for task B, but since the new output layer was initialized randomly it will make large errors (at least during the first few epochs), so there will be large error gradients that may wreck the\n",
    "reused weights. To avoid this, one approach is to freeze the reused layers\n",
    "during the first few epochs, giving the new layer some time to learn\n",
    "reasonable weights. To do this, set every layer's trainable attribute to\n",
    "False and compile the model:\n",
    "\n",
    "```python\n",
    "for layer in model_B_on_A.layers[:-1]: \n",
    "    layer.trainable = False \n",
    " \n",
    "model_B_on_A.compile(loss=\"binary_crossentropy\", optimizer=\"sgd\", \n",
    "                     metrics=[\"accuracy\"])\n",
    "```\n",
    "\n",
    "**Note**:You must always compile your model after you freeze or unfreeze layers.\n",
    "\n",
    "Now you can train the model for a few epochs, then unfreeze the reused\n",
    "layers (which requires compiling the model again) and continue training to\n",
    "fine-tune the reused layers for task B. After unfreezing the reused layers, it is usually a good idea to reduce the learning rate, once again to avoid\n",
    "damaging the reused weights:\n",
    "\n",
    "```python\n",
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=4, \n",
    "                           validation_data=(X_valid_B, y_valid_B)) \n",
    " \n",
    "for layer in model_B_on_A.layers[:-1]: \n",
    "    layer.trainable = True \n",
    " \n",
    "optimizer = keras.optimizers.SGD(lr=1e-4) # the default lr is 1e-2\n",
    "model_B_on_A.compile(loss=\"binary_crossentropy\", optimizer=optimizer, \n",
    "                     metrics=[\"accuracy\"])\n",
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=16, \n",
    "                           validation_data=(X_valid_B, y_valid_B))\n",
    "```\n",
    "\n",
    "This model's test accuracy is 99.25%, which means that transfer learning reduced the error rate from 2.8% down to almost 0.7%! Are you convinced? You shouldn't be: I cheated! I tried many configurations until I found one that demonstrated a strong improvement. Why did I cheat? It turns out that transfer learning does not work very well with small dense networks, presumably because small networks learn few patterns, and dense networks learn very specific patterns, which are unlikely to be useful in other tasks. Transfer learning works best with deep convolutional neural networks, which tend to learn feature detectors that are much more general (especially in the lower layers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Faster Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Five ways to speed up training (and reach a better solution):\n",
    "- applying a good initialization strategy for the connection weights\n",
    "- using a good activation function\n",
    "- using Batch Normalization\n",
    "- reusing parts of a pretrained network\n",
    "- **faster optimizers than SGD**\n",
    "\n",
    "<img src=\"images/TF_FO5.png\" style=\"float:center;\" width=\"500\"/>\n",
    "\n",
    "(* is bad, **is average, and *** is good)\n",
    "\n",
    "Recall SGD is:\n",
    "\n",
    "<img src=\"images/TF_FO0.png\" style=\"float:center;\" width=\"150\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3a. Momentum Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[<img src=\"images/Mom.png\" width=\"500\"/>](https://youtu.be/r-rYz_PEWC8)\n",
    "\n",
    "Recall that Gradient Descent updates the weights by directly subtracting the gradient of the cost function with regard to the weights multiplied by the learning rate. It does not care about what the earlier gradients were. If the local gradient is tiny, it goes very slowly. Momentum optimization cares a great deal about what previous gradients were: at each iteration, it subtracts the local gradient from the _momentum vector_ $m$ (multiplied by the learning rate $\\eta$), and it updates the weights by adding this momentum vector. \n",
    "\n",
    "ACC: In other words, the gradient is used for acceleration, not for speed.  \n",
    "\n",
    "FRICTION: To simulate some sort of friction mechanism and prevent the momentum from growing too large, the algorithm introduces a new hyperparameter $\\beta$, called the _momentum_, which must be set between 0 (high friction) and 1 (no friction). A typical momentum value is 0.9. Due to the momentum, the optimizer may overshoot a bit, then come back, overshoot again, and oscillate like this many times before stabilizing at the minimum. This is one of the reasons it's good to have a bit of friction in the system: it gets rid of these oscillations and thus speeds up convergence.\n",
    "\n",
    "<img src=\"images/TF_FO1.png\" style=\"float:center;\" width=\"200\"/>\n",
    "\n",
    "For example, if $\\beta$ = 0.9, then the terminal velocity is equal to 10 times the gradient times the learning rate, so momentum optimization ends up going 10 times faster than Gradient Descent! This allows momentum optimization to escape from plateaus much faster than Gradient Descent.\n",
    "\n",
    "In deep neural networks that don't use Batch Normalization, the upper layers will often end up having inputs with very different scales, so using momentum optimization helps a lot. It can also help roll past local optima.\n",
    "\n",
    "--- \n",
    "Implementing momentum optimization in Keras:\n",
    "\n",
    "```optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9)```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b. Nesterov Accelerated Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Nesterov Accelerated Gradient (NAG) method, also known as Nesterov momentum optimization, measures the gradient of the cost function not at the local position $\\theta$ but slightly ahead in the direction of the momentum, at $\\theta + \\beta m$. \n",
    "\n",
    "<img src=\"images/TF_FO2.png\" style=\"float:center;\" width=\"250\"/>\n",
    "\n",
    "This small tweak works because in general the momentum vector will be pointing in the right direction (i.e., toward the optimum), so it will be slightly more accurate to use the gradient measured a bit farther in that direction rather than the gradient at the original position. After a while, these small improvements add up and NAG ends up being significantly faster than regular momentum optimization. \n",
    "\n",
    "<img src=\"images/TF_FO2a.png\" style=\"float:center;\" width=\"350\"/>\n",
    "\n",
    "\n",
    "Moreover, note that when the momentum pushes the weights across a valley, $\\nabla_1$ continues to push farther across the valley, while $\\nabla_2$ pushes back toward the bottom of the valley. This helps reduce oscillations and thus NAG converges faster.\n",
    "\n",
    "---\n",
    "NAG is generally faster than regular momentum optimization. To use it,\n",
    "simply set nesterov=True when creating the SGD optimizer:\n",
    "\n",
    "```optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9, nesterov=True)```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3c. AdaGrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GD starts by quickly going down the steepest slope, which may not always point straight toward the global optimum, and then it very slowly goes down to the bottom of the valley. It would be nice if the algorithm could correct its direction earlier to point a bit more toward the global optimum. The AdaGrad algorithm achieves this correction by _scaling_ down the gradient vector along the steepest dimensions. \n",
    "\n",
    "<img src=\"images/TF_FO3.png\" style=\"float:center;\" width=\"250\"/>\n",
    "\n",
    "The first step accumulates the square of the gradients into the vector $s$.  This vectorized form is equivalent to computing s  $ s_i  \\leftarrow  s_i + (\\partial J(\\theta) / \\partial \\theta )$  for each element $s_i$  of the vector $s$; in other words, each $ s_i$  accumulates the squares of the partial derivative of the cost function with regard to parameter $\\theta_i$. If the\n",
    "cost function is steep along the $i^{th}$ dimension, then $s_i$  will get larger and larger at each iteration. The second step is almost identical to Gradient Descent, but with one big difference: the gradient vector is scaled down by a factor of \n",
    "$\\sqrt{s+\\epsilon}$. This vectorized form is equivalent to simultaneously computing $ \\theta_i  \\leftarrow  \\theta_i - \\eta \\ \\partial J(\\theta) / \\partial \\theta_i \\ / \\sqrt{s_i+\\epsilon}$.\n",
    "\n",
    "<img src=\"images/TF_FO3a.png\" style=\"float:center;\" width=\"400\"/>\n",
    "\n",
    "In short, this algorithm decays the learning rate, but it does so faster for steep dimensions than for dimensions with gentler slopes. This is called an _adaptive learning rate_. It helps point the resulting updates more directly toward the global optimum. One additional benefit is that it requires much less tuning of the learning rate hyperparameter $\\eta$.\n",
    "\n",
    "AdaGrad frequently performs well for simple quadratic problems, but it often stops too early when training neural networks. The learning rate gets scaled down so much that the algorithm ends up stopping entirely before reaching the global optimum. So even though Keras has an Adagrad optimizer, you should not use it to train deep neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3d. RMSProp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we've seen, AdaGrad runs the risk of slowing down a bit too fast and never converging to the global optimum. The RMSProp algorithm  fixes this by accumulating only the gradients from the most recent iterations (as opposed to all the gradients since the beginning of training). It does so by using exponential decay in the first step. \n",
    "\n",
    "<img src=\"images/TF_FO4.png\" style=\"float:center;\" width=\"300\"/>\n",
    "\n",
    "Except on very simple problems, this optimizer almost always performs much better than AdaGrad. In fact, it was the preferred optimization algorithm of many researchers until Adam optimization came around.\n",
    "\n",
    "---\n",
    "Keras has an RMSprop optimizer:\n",
    "```optimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9)```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3e. Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adam** which stands for _adaptive moment estimation_, combines the ideas of momentum optimization and RMSProp: just like momentum optimization, it keeps track of an exponentially decaying average of past gradients; and just like RMSProp, it keeps track of an exponentially decaying average of past squared gradients. \n",
    "\n",
    "<img src=\"images/TF_FO6a.png\" style=\"float:center;\" width=\"300\"/>\n",
    "\n",
    "In this equation, T represents the iteration (epoch) number (starting at 1). \n",
    "\n",
    "If you just look at steps 1, 2, and 5, you will notice Adam's close similarity to both momentum optimization and RMSProp. The only difference is that step 1 computes an exponentially decaying average rather than an exponentially decaying sum, but these are actually equivalent except for a constant factor (the decaying average is just $1-\\beta$ times the decaying sum). Steps 3 and 4 are somewhat of a technical detail: since $m$ and $s$ are initialized at 0, they will be biased toward 0 at the beginning of training, so these two steps will help boost $m$ and $s$ at the beginning of training.\n",
    "\n",
    "The momentum decay hyperparameter $\\beta_1$  is typically initialized to 0.9, while the scaling decay hyperparameter $\\beta_2$  is often initialized to 0.999. Since Adam is an adaptive learning rate algorithm (like AdaGrad and RMSProp), it requires less tuning of the learning rate hyperparameter $\\alpha$. You can often use the default value $\\alpha$ = 0.001\n",
    "\n",
    "_AdaMax_: Adam scales down the parameter updates by the square root of $s$. In short, Adam scales down the parameter updates by the $l_2$  norm of the time-decayed gradients. AdaMax replaces the $l_2$  norm with the $l_{\\inf}$ norm (a fancy way of saying the max).\n",
    "\n",
    "_Nadam_: optimization is Adam optimization plus the Nesterov trick, so it\n",
    "will often converge slightly faster than Adam\n",
    "\n",
    "---\n",
    "Adam optimizer using Keras:\n",
    "```optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE: TRAINING SPARSE MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the optimization algorithms just presented produce dense models, meaning that most parameters will be nonzero. If you need a blazingly fast model at runtime, or if you need it to take up less memory, you may prefer to end up with a sparse model instead. \n",
    "\n",
    "One easy way to achieve this is to train the model as usual, then get rid of the tiny weights (set them to zero). Note that this will typically not lead to a very sparse model, and it may degrade the model's performance.\n",
    "\n",
    "A better option is to apply strong $l_1$  regularization during training (we will see how later in this chapter), as it pushes the optimizer to zero out as many weights as it can. \n",
    "\n",
    "If these techniques remain insufficient, check out the [TensorFlow Model Optimization Toolkit (TF-MOT)](https://homl.info/tfmot), which provides a pruning API capable of iteratively removing connections during training based on their magnitude."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE: 1st vs 2nd ORDER DERIVATIVES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the optimization techniques discussed so far only rely on the first-order partial derivatives (Jacobians). The optimization literature also contains amazing algorithms based on the second-order partial derivatives (the Hessians, which are the partial derivatives of the Jacobians). Unfortunately, these algorithms are very hard to apply to deep neural networks because there are $n^2$ Hessians per output (where n is the number of parameters), as opposed to just n Jacobians per output. Since DNNs typically have tens of thousands of parameters, the second-order optimization algorithms often don't even fit in  memory, and even when they do, computing the Hessians is just too slow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Learning Rate Scheduling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optimal Learning Rate**: the optimal learning rate is about half of the maximum learning rate (i.e., the learning rate above which the training algorithm diverges) One way to find a good learning rate is to train the model for a few hundred iterations, starting with a very low learning rate (e.g., 1e-5) and gradually increasing it up to a very large value (e.g., 10). This is done by multiplying the learning rate by a constant factor at each iteration (e.g., by exp(log(1e+6)/500) to go from 1e+5 to 10 in 500\n",
    "iterations). If you plot the loss as a function of the learning rate (using a log scale for the learning rate), you should see it dropping at first. But after a while, the learning rate will be too large, so the loss will shoot back up: the optimal learning rate will be a bit lower than the point at which the loss starts to climb (typically about 10 times lower than the\n",
    "turning point). You can then reinitialize your model and train it normally using this good learning rate.\n",
    "\n",
    "**Learning Schedules**: start with a large learning rate and then reduce it once training stops making fast progress, you can reach a good solution faster than with the optimal constant learning rate. There are many different strategies to reduce the learning rate during training. It can also be beneficial to **_start with a low learning rate, increase it, then drop it again_**. These strategies are called _learning schedules_.\n",
    "\n",
    "<img src=\"images/TF_LR1.jpeg\" style=\"float:center;\" width=\"500\"/>\n",
    "\n",
    "---\n",
    "\n",
    "- Power scheduling in Keras (assumes $c=1$) with $decay=1/s$, ($s$ is the number of steps it takes to divide the\n",
    "learning rate by one more unit)\n",
    "    ```python\n",
    "    optimizer = keras.optimizers.SGD(lr=0.01, decay=1e-4)\n",
    "    ```\n",
    "\n",
    "- Exponential scheduling in Keras using a function and a callback\n",
    "    ```python\n",
    "    def exponential_decay_fn(epoch): \n",
    "        return 0.01 * 0.1**(epoch / 20)\n",
    "    # OR\n",
    "    def exponential_decay(lr0, s): \n",
    "        def exponential_decay_fn(epoch): \n",
    "            return lr0 * 0.1**(epoch / s) \n",
    "        return exponential_decay_fn\n",
    "    exponential_decay_fn = exponential_decay(lr0=0.01, s=20)\n",
    "\n",
    "    lr_scheduler = keras.callbacks.LearningRateScheduler(exponential_decay_fn)\n",
    "    history = model.fit(X_train_scaled, y_train, [...], callbacks=[lr_scheduler])\n",
    "    ```\n",
    "\n",
    "- Piecewise Constant scheduling in Keras using a function and a callback\n",
    "    ```python\n",
    "    def piecewise_constant_fn(epoch): \n",
    "        if epoch < 5: \n",
    "            return 0.01 \n",
    "        elif epoch < 15: \n",
    "            return 0.005 \n",
    "        else: \n",
    "            return 0.001\n",
    "\n",
    "    lr_scheduler = keras.callbacks.LearningRateScheduler(piecewise_constant_fn)\n",
    "    history = model.fit(X_train_scaled, y_train, [...], callbacks=[lr_scheduler])\n",
    "    ```\n",
    "\n",
    "- Performance scheduling in Keras using a ```ReduceLROnPlateau``` callback\n",
    "    ```python\n",
    "    lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)\n",
    "    history = model.fit(X_train_scaled, y_train, [...], callbacks=[lr_scheduler])\n",
    "    ```\n",
    "\n",
    "\n",
    "**Saving and Loading**: When you save a model, the optimizer and its learning rate get saved along with it. This means that with this new schedule function, you could just load a trained model and continue training where it left off, no problem. Things\n",
    "are not so simple if your schedule function uses the epoch argument however: the epoch does not get saved, and it gets reset to 0 every time you call the fit() method. If you were to continue training a model where it left off, this could lead to a very large learning rate, which would likely damage your model's weights. One solution is to manually set the fit() method's ```initial_epoch``` argument so the epoch starts at the right value. \n",
    "\n",
    "**keras.optimizers.schedules**: _tf.keras_ (and not _keras_) offers an alternative way to implement learning rate scheduling: define the learning rate using one of the schedules available in ```keras.optimizers.schedules```, then pass this learning rate to any\n",
    "optimizer. This approach updates the learning rate at each step rather than at each epoch.\n",
    "\n",
    "```python\n",
    "s = 20 * len(X_train) // 32 # number of steps in 20 epochs (batch size = 32)\n",
    "learning_rate = keras.optimizers.schedules.ExponentialDecay(0.01, s, 0.1)\n",
    "optimizer = keras.optimizers.SGD(learning_rate) # or any optimizer\n",
    "```\n",
    "\n",
    "This is nice and simple, plus when you save the model, the learning rate and its schedule (including its state) get saved as well. This approach, however, is not part of the Keras API; it is specific to tf.keras.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Avoiding Overfitting Through Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5a. $l_1$ and $l_2$ Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[<img src=\"images/Reg.png\" width=\"500\"/>](https://youtu.be/ndYnUrx8xvs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$l_2$ regularization to a Keras layer's connection weights, using a regularization factor of 0.01:\n",
    "\n",
    "```python\n",
    "layer = keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\", \n",
    "                           kernel_regularizer=keras.regularizers.l2(0.01))\n",
    "```\n",
    "\n",
    "The l2() function returns a regularizer that will be called at each step during training to compute the regularization loss. This is then added to the final loss. As you might expect, you can just use ```keras.regularizers.l1()``` if you want $l_!$   regularization; if you want both $l_1$  and $l_2$  regularization, use ```keras.regularizers.l1_l2()``` (specifying both regularization factors).\n",
    "\n",
    "Apply the same regularizer to all layers in your network, as well as using the same activation function and the same\n",
    "initialization strategy in all hidden layers:\n",
    "\n",
    "```python\n",
    "from functools import partial \n",
    " \n",
    "RegularizedDense = partial(keras.layers.Dense, \n",
    "                           activation=\"elu\", \n",
    "                           kernel_initializer=\"he_normal\", \n",
    "                           kernel_regularizer=keras.regularizers.l2(0.01)) \n",
    " \n",
    "model = keras.models.Sequential([ \n",
    "    keras.layers.Flatten(input_shape=[28, 28]), \n",
    "    RegularizedDense(300), \n",
    "    RegularizedDense(100), \n",
    "    RegularizedDense(10, activation=\"softmax\", \n",
    "                     kernel_initializer=\"glorot_uniform\")\n",
    "])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5b. Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At every training step, every neuron (including the input neurons, but always excluding the output neurons) has a probability $p$ of being temporarily \"dropped out\", meaning it will be entirely ignored during this training step, but it may be active during the next step. The hyperparameter $p$ is called the _dropout rate_, and it is typically set between 10% and 50%: closer to 20-30% in RNNs and closer to 40-50% in CNNs. After training, neurons don't get dropped anymore.\n",
    "\n",
    "Droupout basically allow NN to **not** depend on any one feature and spreads out the weights. Neurons trained with dropout cannot co-adapt with their neighboring neurons; they have to be as useful as possible on their own. They also cannot rely excessively on just a few input neurons; they must pay attention to each of their input neurons. They end up being less sensitive to slight changes in the inputs. In the end, you get a more robust network that generalizes better.\n",
    "\n",
    "Another way to understand the power of dropout is to realize that a unique neural network is generated at each training step. Since each neuron can be either present or absent, there are a total of 2  possible networks (where N is the total number of droppable neurons). This is such a huge number that it is virtually impossible for the same neural network to be sampled twice.\n",
    "Once you have run 10,000 training steps, you have essentially trained 10,000 different neural networks (each with just one training instance). These neural networks are obviously not independent because they share many of their weights, but they are nevertheless all different. The resulting neural network can be seen as an **averaging ensemble** of all these smaller neural networks.\n",
    "\n",
    "Suppose $p = 25%$ , in which case during testing a neuron would be connected to 4/3 times as many input neurons as it would be (on average) during training. To compensate for this fact, we need to multiply each neuron's input connection weights by 3/4 after training. If we don't, each neuron will get a total input signal roughly 4/3 times as large as what the network was trained on and will be unlikely to perform well. More generally, we need to multiply each input connection weight by the _keep probability_ $(1-p)$ after training. Alternatively, we can divide each neuron's output by the _keep probability_ during training (these alternatives are not perfectly equivalent, but they work equally well).\n",
    "\n",
    "**NOTE**: In practice, you can usually apply dropout only to the neurons in the top 1-3 layers (excluding the output layer). Moreover, many state-of-the-art architectures only use dropout after the last hidden layer, so you may want to try this if full dropout is too strong.\n",
    "\n",
    "**NOTE**: Since dropout is only active during training, comparing the training loss and the validation loss can be misleading. In particular, a model may be overfitting the training set and yet have similar training and validation losses. So make sure to evaluate the training loss without dropout (e.g., after training).\n",
    "\n",
    "**NOTE**: If you want to regularize a self-normalizing network based on the SELU activation function (as discussed earlier), you should use alpha dropout: this is a variant of dropout that preserves the mean and standard deviation of its inputs.\n",
    "\n",
    "---\n",
    "Implement dropout using Keras\n",
    "```python\n",
    "model = keras.models.Sequential([ \n",
    "    keras.layers.Flatten(input_shape=[28, 28]), \n",
    "    keras.layers.Dropout(rate=0.2), \n",
    "    keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"), \n",
    "    keras.layers.Dropout(rate=0.2), \n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"), \n",
    "    keras.layers.Dropout(rate=0.2), \n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "```\n",
    "During training, keras.layers.Dropout layer randomly drops some inputs (setting them to 0) and divides the remaining inputs by the _keep probability_ (```rate```). After training, it does nothing at all; it just passes the inputs to the next layer. \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5c. Monte Carlo (MC) Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boost the performance of any trained dropout model without having to retrain it or even modify it at all, provides a much better measure of the model's uncertainty, and is also amazingly simple to implement. \n",
    "\n",
    "We just make 100 predictions over the test set, setting ```training=True``` to ensure that the Dropout layer is active, and stack the predictions. Since dropout is active, all the predictions will be different. ```y_probas``` is an array of shape [100, 10000, 10] for 100 smaples, 10000 instances and 10 classes. Once we average over the first dimension (axis=0), we get ```y_proba```, an array of shape [10000, 10], like we would get with a single prediction.\n",
    "\n",
    "```python\n",
    "y_probas = np.stack([model(X_test_scaled, training=True) \n",
    "                     for sample in range(100)])\n",
    "y_proba = y_probas.mean(axis=0)\n",
    "```\n",
    "\n",
    "with dropout off: \n",
    "```python\n",
    "np.round(model.predict(X_test_scaled[:1]), 2)\n",
    "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.99]], dtype=float32)\n",
    "```\n",
    "\n",
    "with dropout on (```training=True```): \n",
    "```python\n",
    "np.round(y_probas[:, :1], 2)\n",
    "array([[[0.  , 0.  , 0.  , 0.  , 0.  , 0.14, 0.  , 0.17, 0.  , 0.68]],\n",
    "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.16, 0.  , 0.2 , 0.  , 0.64]],\n",
    "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.01, 0.  , 0.97]],\n",
    "       [...]\n",
    "```\n",
    "\n",
    "apparently, when we activate dropout, the model is not sure anymore. Once we average over the first dimension, we get the\n",
    "following MC Dropout predictions:\n",
    "\n",
    "```python\n",
    "np.round(y_proba[:1], 2)\n",
    "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.22, 0.  , 0.16, 0.  , 0.62]], dtype=float32)\n",
    "```\n",
    "\n",
    "The model still thinks this image belongs to class 9, but only with a 62% confidence, which seems much more reasonable than 99%. Plus it's useful to know exactly which other classes it thinks are likely. \n",
    "\n",
    "take a look at the standard deviation of the probability estimates:\n",
    "```python\n",
    "y_std = y_probas.std(axis=0)\n",
    "np.round(y_std[:1], 2)\n",
    "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.28, 0.  , 0.21, 0.02, 0.32]],\n",
    "      dtype=float32)\n",
    "```\n",
    "\n",
    "Apparently there's quite a lot of variance in the probability estimates: if you were building a risk-sensitive system (e.g., a medical or financial system), you should probably treat such an uncertain prediction with extreme caution. You definitely would not treat it like a 99% confident prediction.\n",
    "\n",
    "In short, MC Dropout is a fantastic technique that boosts dropout models and provides better uncertainty estimates. And of course, since it is just regular dropout during training, it also acts like a regularizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5d. Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data augmentation artificially increases the size of the training set by generating many realistic variants of each training instance. This reduces overfitting, making this a regularization technique. The generated instances should be as realistic as possible: ideally, given an image from the augmented training set, a human should not be able to tell whether it was augmented or not. **Simply adding white noise will not help; the modifications should be learnable (white noise is not)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Understanding Neural Noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[<img src=\"images/Noise.png\" width=\"600\"/>](https://youtu.be/ubqhh4Iv7O4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Understanding Inefficiencies in our Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[<img src=\"images/Inefficiency.png\" width=\"600\"/>](https://youtu.be/ubqhh4Iv7O4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following configuration generally works fine in most cases, without requiring much hyperparameter tuning.\n",
    "\n",
    "<img src=\"images/TF_TrSumm1.png\" style=\"float:left;\" width=\"400\"/>\n",
    "\n",
    "<img src=\"images/TF_TrSumm2.png\" style=\"float:left;\" width=\"404\"/>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Exercise: Build a DNN with 20 hidden layers of 100 neurons each (that's too many, but it's the point of this exercise). Use He initialization and the ELU activation function. input_shape=[32, 32, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(Dense(100, activation='elu', kernel_initializer='he_uniform'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "b. \n",
    "Exercise: Using Nadam optimization and early stopping, train the network on the CIFAR10 dataset. You can load it with keras.datasets.cifar10.load_data(). The dataset is composed of 60,000 32 × 32--pixel color images (50,000 for training, 10,000 for testing) with 10 classes, so you'll need a softmax output layer with 10 neurons. Remember to search for the right learning rate each time you change the model's architecture or hyperparameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = \\\n",
    "    tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "X_train = X_train_full[5000:]\n",
    "y_train = y_train_full[5000:]\n",
    "X_valid = X_train_full[:5000]\n",
    "y_valid = y_train_full[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(10, activation='softmax'))\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", \n",
    "              optimizer=tf.keras.optimizers.Nadam(lr=5e-3),\n",
    "              metrics='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "45/45 [==============================] - 5s 29ms/step - loss: 641.5301 - accuracy: 0.0962 - val_loss: 4.5043 - val_accuracy: 0.0916\n",
      "Epoch 2/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 3.4823 - accuracy: 0.1115 - val_loss: 2.1536 - val_accuracy: 0.1954\n",
      "Epoch 3/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 2.1847 - accuracy: 0.1862 - val_loss: 2.1556 - val_accuracy: 0.1960\n",
      "Epoch 4/100\n",
      "45/45 [==============================] - 1s 20ms/step - loss: 2.1431 - accuracy: 0.2006 - val_loss: 2.1369 - val_accuracy: 0.1898\n",
      "Epoch 5/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 2.1330 - accuracy: 0.2041 - val_loss: 2.0621 - val_accuracy: 0.2286\n",
      "Epoch 6/100\n",
      "45/45 [==============================] - 1s 20ms/step - loss: 2.0839 - accuracy: 0.2280 - val_loss: 2.0620 - val_accuracy: 0.2106\n",
      "Epoch 7/100\n",
      "45/45 [==============================] - 1s 20ms/step - loss: 2.0802 - accuracy: 0.2260 - val_loss: 1.9850 - val_accuracy: 0.2504\n",
      "Epoch 8/100\n",
      "45/45 [==============================] - 1s 20ms/step - loss: 2.0565 - accuracy: 0.2422 - val_loss: 2.0769 - val_accuracy: 0.2154\n",
      "Epoch 9/100\n",
      "45/45 [==============================] - 1s 20ms/step - loss: 2.0385 - accuracy: 0.2432 - val_loss: 2.1100 - val_accuracy: 0.2200\n",
      "Epoch 10/100\n",
      "45/45 [==============================] - 1s 20ms/step - loss: 1.9983 - accuracy: 0.2644 - val_loss: 1.9697 - val_accuracy: 0.2654\n",
      "Epoch 11/100\n",
      "45/45 [==============================] - 1s 21ms/step - loss: 1.9761 - accuracy: 0.2700 - val_loss: 2.0390 - val_accuracy: 0.2562\n",
      "Epoch 12/100\n",
      "45/45 [==============================] - 1s 20ms/step - loss: 1.9845 - accuracy: 0.2736 - val_loss: 1.8991 - val_accuracy: 0.3044\n",
      "Epoch 13/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.9389 - accuracy: 0.2947 - val_loss: 1.8749 - val_accuracy: 0.3128\n",
      "Epoch 14/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.9331 - accuracy: 0.2985 - val_loss: 1.8315 - val_accuracy: 0.3356\n",
      "Epoch 15/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.9167 - accuracy: 0.3075 - val_loss: 1.8659 - val_accuracy: 0.3168\n",
      "Epoch 16/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.9315 - accuracy: 0.2946 - val_loss: 1.9130 - val_accuracy: 0.2976\n",
      "Epoch 17/100\n",
      "45/45 [==============================] - 1s 18ms/step - loss: 1.8985 - accuracy: 0.3106 - val_loss: 2.0727 - val_accuracy: 0.2442\n",
      "Epoch 18/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.9359 - accuracy: 0.2933 - val_loss: 1.8344 - val_accuracy: 0.3300\n",
      "Epoch 19/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.8751 - accuracy: 0.3218 - val_loss: 1.8338 - val_accuracy: 0.3392\n",
      "Epoch 20/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.8829 - accuracy: 0.3176 - val_loss: 1.8033 - val_accuracy: 0.3434\n",
      "Epoch 21/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.8323 - accuracy: 0.3363 - val_loss: 1.7898 - val_accuracy: 0.3526\n",
      "Epoch 22/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.8329 - accuracy: 0.3360 - val_loss: 1.8436 - val_accuracy: 0.3320\n",
      "Epoch 23/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.8316 - accuracy: 0.3364 - val_loss: 1.8691 - val_accuracy: 0.3224\n",
      "Epoch 24/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.8293 - accuracy: 0.3366 - val_loss: 1.8075 - val_accuracy: 0.3486\n",
      "Epoch 25/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.8420 - accuracy: 0.3336 - val_loss: 1.7651 - val_accuracy: 0.3636\n",
      "Epoch 26/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.7968 - accuracy: 0.3480 - val_loss: 1.8123 - val_accuracy: 0.3436\n",
      "Epoch 27/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.8106 - accuracy: 0.3492 - val_loss: 1.7388 - val_accuracy: 0.3664\n",
      "Epoch 28/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.7869 - accuracy: 0.3539 - val_loss: 1.7652 - val_accuracy: 0.3508\n",
      "Epoch 29/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.7954 - accuracy: 0.3488 - val_loss: 1.7516 - val_accuracy: 0.3618\n",
      "Epoch 30/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.7843 - accuracy: 0.3538 - val_loss: 1.8811 - val_accuracy: 0.3316\n",
      "Epoch 31/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.7962 - accuracy: 0.3565 - val_loss: 1.7497 - val_accuracy: 0.3766\n",
      "Epoch 32/100\n",
      "45/45 [==============================] - 1s 20ms/step - loss: 1.7821 - accuracy: 0.3608 - val_loss: 1.7305 - val_accuracy: 0.3658\n",
      "Epoch 33/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.7809 - accuracy: 0.3586 - val_loss: 1.9279 - val_accuracy: 0.3160\n",
      "Epoch 34/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.7836 - accuracy: 0.3563 - val_loss: 1.7769 - val_accuracy: 0.3614\n",
      "Epoch 35/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.7817 - accuracy: 0.3580 - val_loss: 1.7969 - val_accuracy: 0.3620\n",
      "Epoch 36/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.7807 - accuracy: 0.3573 - val_loss: 1.7437 - val_accuracy: 0.3620\n",
      "Epoch 37/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.7998 - accuracy: 0.3495 - val_loss: 1.7271 - val_accuracy: 0.3748\n",
      "Epoch 38/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.7520 - accuracy: 0.3688 - val_loss: 1.7675 - val_accuracy: 0.3690\n",
      "Epoch 39/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.7361 - accuracy: 0.3752 - val_loss: 1.6985 - val_accuracy: 0.3876\n",
      "Epoch 40/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.7340 - accuracy: 0.3726 - val_loss: 1.7722 - val_accuracy: 0.3506\n",
      "Epoch 41/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.7410 - accuracy: 0.3668 - val_loss: 1.7653 - val_accuracy: 0.3620\n",
      "Epoch 42/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.7506 - accuracy: 0.3659 - val_loss: 1.7218 - val_accuracy: 0.3746\n",
      "Epoch 43/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.7386 - accuracy: 0.3716 - val_loss: 1.7659 - val_accuracy: 0.3544\n",
      "Epoch 44/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.7340 - accuracy: 0.3702 - val_loss: 1.7388 - val_accuracy: 0.3770\n",
      "Epoch 45/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.7444 - accuracy: 0.3724 - val_loss: 1.7281 - val_accuracy: 0.3758\n",
      "Epoch 46/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.7170 - accuracy: 0.3770 - val_loss: 1.7849 - val_accuracy: 0.3500\n",
      "Epoch 47/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.7302 - accuracy: 0.3731 - val_loss: 1.6807 - val_accuracy: 0.3880\n",
      "Epoch 48/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.7042 - accuracy: 0.3818 - val_loss: 1.7119 - val_accuracy: 0.3820\n",
      "Epoch 49/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.7108 - accuracy: 0.3821 - val_loss: 1.7574 - val_accuracy: 0.3582\n",
      "Epoch 50/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.7189 - accuracy: 0.3802 - val_loss: 1.6818 - val_accuracy: 0.3946\n",
      "Epoch 51/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.7042 - accuracy: 0.3783 - val_loss: 1.8044 - val_accuracy: 0.3486\n",
      "Epoch 52/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.7258 - accuracy: 0.3770 - val_loss: 1.7129 - val_accuracy: 0.3812\n",
      "Epoch 53/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.7006 - accuracy: 0.3858 - val_loss: 1.7486 - val_accuracy: 0.3692\n",
      "Epoch 54/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.6909 - accuracy: 0.3925 - val_loss: 1.7508 - val_accuracy: 0.3624\n",
      "Epoch 55/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.6944 - accuracy: 0.3875 - val_loss: 1.7355 - val_accuracy: 0.3764\n",
      "Epoch 56/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.6928 - accuracy: 0.3878 - val_loss: 1.6628 - val_accuracy: 0.4042\n",
      "Epoch 57/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.6820 - accuracy: 0.3920 - val_loss: 1.7349 - val_accuracy: 0.3718\n",
      "Epoch 58/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.7054 - accuracy: 0.3842 - val_loss: 1.6895 - val_accuracy: 0.3914\n",
      "Epoch 59/100\n",
      "45/45 [==============================] - 1s 20ms/step - loss: 1.6976 - accuracy: 0.3895 - val_loss: 1.6784 - val_accuracy: 0.3854\n",
      "Epoch 60/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.6909 - accuracy: 0.3878 - val_loss: 1.7172 - val_accuracy: 0.3756\n",
      "Epoch 61/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.6833 - accuracy: 0.3897 - val_loss: 1.6582 - val_accuracy: 0.4020\n",
      "Epoch 62/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.6631 - accuracy: 0.3990 - val_loss: 1.7311 - val_accuracy: 0.3804\n",
      "Epoch 63/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.6937 - accuracy: 0.3942 - val_loss: 1.7370 - val_accuracy: 0.3618\n",
      "Epoch 64/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.6668 - accuracy: 0.3978 - val_loss: 1.7202 - val_accuracy: 0.3838\n",
      "Epoch 65/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.6752 - accuracy: 0.3940 - val_loss: 1.6623 - val_accuracy: 0.3986\n",
      "Epoch 66/100\n",
      "45/45 [==============================] - 1s 20ms/step - loss: 1.6786 - accuracy: 0.3921 - val_loss: 1.6875 - val_accuracy: 0.3862\n",
      "Epoch 67/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.6609 - accuracy: 0.3982 - val_loss: 1.6948 - val_accuracy: 0.3916\n",
      "Epoch 68/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.6796 - accuracy: 0.3907 - val_loss: 1.7530 - val_accuracy: 0.3612\n",
      "Epoch 69/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.6550 - accuracy: 0.4017 - val_loss: 1.7043 - val_accuracy: 0.3840\n",
      "Epoch 70/100\n",
      "45/45 [==============================] - 1s 20ms/step - loss: 1.6515 - accuracy: 0.4011 - val_loss: 1.6804 - val_accuracy: 0.3860\n",
      "Epoch 71/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.6607 - accuracy: 0.4002 - val_loss: 1.7028 - val_accuracy: 0.3874\n",
      "Epoch 72/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.6744 - accuracy: 0.3947 - val_loss: 1.6928 - val_accuracy: 0.3814\n",
      "Epoch 73/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.6534 - accuracy: 0.4031 - val_loss: 1.6770 - val_accuracy: 0.3996\n",
      "Epoch 74/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.6252 - accuracy: 0.4088 - val_loss: 1.7551 - val_accuracy: 0.3628\n",
      "Epoch 75/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.6387 - accuracy: 0.4034 - val_loss: 1.6738 - val_accuracy: 0.3982\n",
      "Epoch 76/100\n",
      "45/45 [==============================] - 1s 20ms/step - loss: 1.6438 - accuracy: 0.4052 - val_loss: 1.6808 - val_accuracy: 0.3862\n",
      "Epoch 77/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.6486 - accuracy: 0.4012 - val_loss: 1.6454 - val_accuracy: 0.3960\n",
      "Epoch 78/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.6350 - accuracy: 0.4079 - val_loss: 1.7133 - val_accuracy: 0.3930\n",
      "Epoch 79/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.6510 - accuracy: 0.3994 - val_loss: 1.7034 - val_accuracy: 0.3772\n",
      "Epoch 80/100\n",
      "45/45 [==============================] - 1s 20ms/step - loss: 1.6216 - accuracy: 0.4118 - val_loss: 1.6681 - val_accuracy: 0.4062\n",
      "Epoch 81/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.6488 - accuracy: 0.4044 - val_loss: 1.7819 - val_accuracy: 0.3606\n",
      "Epoch 82/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.6438 - accuracy: 0.4051 - val_loss: 1.7249 - val_accuracy: 0.3774\n",
      "Epoch 83/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.6073 - accuracy: 0.4197 - val_loss: 1.6332 - val_accuracy: 0.4162\n",
      "Epoch 84/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.6165 - accuracy: 0.4122 - val_loss: 1.6812 - val_accuracy: 0.3898\n",
      "Epoch 85/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.6309 - accuracy: 0.4101 - val_loss: 1.6528 - val_accuracy: 0.4128\n",
      "Epoch 86/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.6109 - accuracy: 0.4180 - val_loss: 1.6656 - val_accuracy: 0.3964\n",
      "Epoch 87/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.6113 - accuracy: 0.4153 - val_loss: 1.6941 - val_accuracy: 0.3836\n",
      "Epoch 88/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.6091 - accuracy: 0.4162 - val_loss: 1.7468 - val_accuracy: 0.3760\n",
      "Epoch 89/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.6302 - accuracy: 0.4117 - val_loss: 1.6657 - val_accuracy: 0.4054\n",
      "Epoch 90/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.6088 - accuracy: 0.4173 - val_loss: 1.7401 - val_accuracy: 0.3846\n",
      "Epoch 91/100\n",
      "45/45 [==============================] - 1s 20ms/step - loss: 1.6064 - accuracy: 0.4231 - val_loss: 1.6465 - val_accuracy: 0.4052\n",
      "Epoch 92/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.6011 - accuracy: 0.4232 - val_loss: 1.6644 - val_accuracy: 0.3950\n",
      "Epoch 93/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.5990 - accuracy: 0.4231 - val_loss: 1.6623 - val_accuracy: 0.4052\n",
      "Epoch 94/100\n",
      "45/45 [==============================] - 1s 20ms/step - loss: 1.6217 - accuracy: 0.4150 - val_loss: 1.6880 - val_accuracy: 0.3924\n",
      "Epoch 95/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.6072 - accuracy: 0.4186 - val_loss: 1.6817 - val_accuracy: 0.3904\n",
      "Epoch 96/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.5714 - accuracy: 0.4329 - val_loss: 1.6472 - val_accuracy: 0.4078\n",
      "Epoch 97/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.6159 - accuracy: 0.4192 - val_loss: 1.7011 - val_accuracy: 0.3840\n",
      "Epoch 98/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.6043 - accuracy: 0.4247 - val_loss: 1.6279 - val_accuracy: 0.4084\n",
      "Epoch 99/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.6010 - accuracy: 0.4215 - val_loss: 1.6328 - val_accuracy: 0.4118\n",
      "Epoch 100/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.6091 - accuracy: 0.4173 - val_loss: 1.6507 - val_accuracy: 0.4082\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2ad11e9a90a0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=20)\n",
    "model_checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "    'modules/my_cifar10_model.h5', save_best_only=True)\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb]\n",
    "\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=1000,\n",
    "          validation_data=(X_valid, y_valid), validation_batch_size=1000,\n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 1s 3ms/step - loss: 1.6279 - accuracy: 0.4084\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.6279069185256958, 0.4083999991416931]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.models.load_model(\"modules/my_cifar10_model.h5\")\n",
    "model.evaluate(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Now try adding Batch Normalization and compare the learning curves: Is it converging faster than before? Does it produce a better model? How does it affect training speed?\n",
    "\n",
    "The code below is very similar to the code above, with a few changes:\n",
    "\n",
    "    I added a BN layer after every Dense layer (before the activation function), except for the output layer. I also added a BN layer before the first hidden layer.\n",
    "    I changed the learning rate to 5e-4. I experimented with 1e-5, 3e-5, 5e-5, 1e-4, 3e-4, 5e-4, 1e-3 and 3e-3, and I chose the one with the best validation performance after 20 epochs.\n",
    "    I renamed the run directories to runbn* and the model file name to my_cifar10_bn_model.h5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "45/45 [==============================] - 14s 63ms/step - loss: 2.3692 - accuracy: 0.2054 - val_loss: 3.9958 - val_accuracy: 0.1018\n",
      "Epoch 2/100\n",
      "45/45 [==============================] - 2s 44ms/step - loss: 1.7140 - accuracy: 0.3856 - val_loss: 2.5236 - val_accuracy: 0.2422\n",
      "Epoch 3/100\n",
      "45/45 [==============================] - 2s 43ms/step - loss: 1.5670 - accuracy: 0.4420 - val_loss: 2.0447 - val_accuracy: 0.3556\n",
      "Epoch 4/100\n",
      "45/45 [==============================] - 2s 43ms/step - loss: 1.4681 - accuracy: 0.4768 - val_loss: 1.7554 - val_accuracy: 0.4120\n",
      "Epoch 5/100\n",
      "45/45 [==============================] - 2s 43ms/step - loss: 1.3868 - accuracy: 0.5062 - val_loss: 1.6572 - val_accuracy: 0.4386\n",
      "Epoch 6/100\n",
      "45/45 [==============================] - 2s 44ms/step - loss: 1.3236 - accuracy: 0.5317 - val_loss: 1.5914 - val_accuracy: 0.4488\n",
      "Epoch 7/100\n",
      "45/45 [==============================] - 2s 41ms/step - loss: 1.2631 - accuracy: 0.5543 - val_loss: 1.5548 - val_accuracy: 0.4542\n",
      "Epoch 8/100\n",
      "45/45 [==============================] - 2s 41ms/step - loss: 1.2180 - accuracy: 0.5667 - val_loss: 1.5881 - val_accuracy: 0.4514\n",
      "Epoch 9/100\n",
      "45/45 [==============================] - 2s 41ms/step - loss: 1.1686 - accuracy: 0.5844 - val_loss: 1.5558 - val_accuracy: 0.4678\n",
      "Epoch 10/100\n",
      "45/45 [==============================] - 2s 41ms/step - loss: 1.1326 - accuracy: 0.6016 - val_loss: 1.5638 - val_accuracy: 0.4614\n",
      "Epoch 11/100\n",
      "45/45 [==============================] - 2s 41ms/step - loss: 1.1003 - accuracy: 0.6109 - val_loss: 1.5755 - val_accuracy: 0.4596\n",
      "Epoch 12/100\n",
      "45/45 [==============================] - 2s 42ms/step - loss: 1.0445 - accuracy: 0.6291 - val_loss: 1.6420 - val_accuracy: 0.4574\n",
      "Epoch 13/100\n",
      "45/45 [==============================] - 2s 43ms/step - loss: 1.0171 - accuracy: 0.6419 - val_loss: 1.6282 - val_accuracy: 0.4560\n",
      "Epoch 14/100\n",
      "45/45 [==============================] - 2s 43ms/step - loss: 0.9661 - accuracy: 0.6586 - val_loss: 1.6520 - val_accuracy: 0.4612\n",
      "Epoch 15/100\n",
      "45/45 [==============================] - 2s 44ms/step - loss: 0.9346 - accuracy: 0.6737 - val_loss: 1.6934 - val_accuracy: 0.4560\n",
      "Epoch 16/100\n",
      "45/45 [==============================] - 2s 44ms/step - loss: 0.8901 - accuracy: 0.6845 - val_loss: 1.6843 - val_accuracy: 0.4622\n",
      "Epoch 17/100\n",
      "45/45 [==============================] - 2s 44ms/step - loss: 0.8566 - accuracy: 0.6992 - val_loss: 1.7657 - val_accuracy: 0.4514\n",
      "Epoch 18/100\n",
      "45/45 [==============================] - 2s 44ms/step - loss: 0.8279 - accuracy: 0.7100 - val_loss: 1.7753 - val_accuracy: 0.4500\n",
      "Epoch 19/100\n",
      "45/45 [==============================] - 2s 44ms/step - loss: 0.7773 - accuracy: 0.7275 - val_loss: 1.8009 - val_accuracy: 0.4572\n",
      "Epoch 20/100\n",
      "45/45 [==============================] - 2s 43ms/step - loss: 0.7689 - accuracy: 0.7323 - val_loss: 1.8146 - val_accuracy: 0.4546\n",
      "Epoch 21/100\n",
      "45/45 [==============================] - 2s 43ms/step - loss: 0.7401 - accuracy: 0.7408 - val_loss: 1.8886 - val_accuracy: 0.4550\n",
      "Epoch 22/100\n",
      "45/45 [==============================] - 2s 44ms/step - loss: 0.6896 - accuracy: 0.7640 - val_loss: 1.9243 - val_accuracy: 0.4520\n",
      "Epoch 23/100\n",
      "45/45 [==============================] - 2s 43ms/step - loss: 0.6613 - accuracy: 0.7724 - val_loss: 1.9891 - val_accuracy: 0.4430\n",
      "Epoch 24/100\n",
      "45/45 [==============================] - 2s 45ms/step - loss: 0.6312 - accuracy: 0.7816 - val_loss: 2.0032 - val_accuracy: 0.4528\n",
      "Epoch 25/100\n",
      "45/45 [==============================] - 2s 44ms/step - loss: 0.5986 - accuracy: 0.7920 - val_loss: 2.0890 - val_accuracy: 0.4420\n",
      "Epoch 26/100\n",
      "45/45 [==============================] - 2s 44ms/step - loss: 0.5862 - accuracy: 0.7959 - val_loss: 2.0663 - val_accuracy: 0.4540\n",
      "Epoch 27/100\n",
      "45/45 [==============================] - 2s 44ms/step - loss: 0.5453 - accuracy: 0.8131 - val_loss: 2.1606 - val_accuracy: 0.4476\n",
      "157/157 [==============================] - 2s 5ms/step - loss: 1.5548 - accuracy: 0.4542\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.5548253059387207, 0.45419999957084656]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=[32, 32, 3]))\n",
    "model.add(BatchNormalization())\n",
    "for _ in range(20):\n",
    "    model.add(Dense(100, activation='elu', kernel_initializer='he_uniform'))\n",
    "    model.add(BatchNormalization())\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", \n",
    "              optimizer=tf.keras.optimizers.Nadam(lr=5e-4),\n",
    "              metrics='accuracy')\n",
    "\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=20)\n",
    "model_checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "    'modules/my_cifar10_bn_model.h5', save_best_only=True)\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb]\n",
    "\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=1000,\n",
    "          validation_data=(X_valid, y_valid), validation_batch_size=1000,\n",
    "          callbacks=callbacks)\n",
    "\n",
    "model = tf.keras.models.load_model(\"modules/my_cifar10_bn_model.h5\")\n",
    "model.evaluate(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Is the model converging faster than before? Much faster! The previous model took 27 epochs to reach the lowest validation loss, while the new model achieved that same loss in just 5 epochs and continued to make progress until the 16th epoch. The BN layers stabilized training and allowed us to use a much larger learning rate, so convergence was faster.\n",
    "- Does BN produce a better model? Yes! The final model is also much better, with 45.0% accuracy instead of 40%. It's still not a very good model, but at least it's much better than before (a Convolutional Neural Network would do much better, but that's a different topic).\n",
    "- How does BN affect training speed? Although the model converged much faster, each epoch took about 12s instead of 8s, because of the extra computations required by the BN layers. But overall the training time (wall time) was shortened significantly!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. Try replacing Batch Normalization with SELU, and make the necessary adjustements to ensure the network self-normalizes (i.e., standardize the input features, use LeCun normal initialization, make sure the DNN contains only a sequence of dense layers, etc.).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "45/45 [==============================] - 5s 29ms/step - loss: 2.8891 - accuracy: 0.1155 - val_loss: 2.1930 - val_accuracy: 0.1782\n",
      "Epoch 2/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 2.2107 - accuracy: 0.1830 - val_loss: 2.0390 - val_accuracy: 0.2582\n",
      "Epoch 3/100\n",
      "45/45 [==============================] - 1s 20ms/step - loss: 2.0963 - accuracy: 0.2208 - val_loss: 2.0382 - val_accuracy: 0.2438\n",
      "Epoch 4/100\n",
      "45/45 [==============================] - 1s 20ms/step - loss: 2.0475 - accuracy: 0.2441 - val_loss: 1.9646 - val_accuracy: 0.2584\n",
      "Epoch 5/100\n",
      "45/45 [==============================] - 1s 20ms/step - loss: 1.9804 - accuracy: 0.2730 - val_loss: 1.9485 - val_accuracy: 0.2726\n",
      "Epoch 6/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.9198 - accuracy: 0.3029 - val_loss: 1.9218 - val_accuracy: 0.3018\n",
      "Epoch 7/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.8885 - accuracy: 0.3188 - val_loss: 1.7956 - val_accuracy: 0.3508\n",
      "Epoch 8/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.8394 - accuracy: 0.3351 - val_loss: 1.7828 - val_accuracy: 0.3536\n",
      "Epoch 9/100\n",
      "45/45 [==============================] - 1s 18ms/step - loss: 1.8462 - accuracy: 0.3295 - val_loss: 1.9272 - val_accuracy: 0.2986\n",
      "Epoch 10/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.8220 - accuracy: 0.3437 - val_loss: 1.7623 - val_accuracy: 0.3630\n",
      "Epoch 11/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.7772 - accuracy: 0.3571 - val_loss: 1.8268 - val_accuracy: 0.3426\n",
      "Epoch 12/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.7819 - accuracy: 0.3564 - val_loss: 1.7436 - val_accuracy: 0.3634\n",
      "Epoch 13/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.7892 - accuracy: 0.3557 - val_loss: 1.7664 - val_accuracy: 0.3594\n",
      "Epoch 14/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.7272 - accuracy: 0.3765 - val_loss: 1.7701 - val_accuracy: 0.3610\n",
      "Epoch 15/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.7273 - accuracy: 0.3784 - val_loss: 1.7624 - val_accuracy: 0.3634\n",
      "Epoch 16/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.7263 - accuracy: 0.3749 - val_loss: 1.7064 - val_accuracy: 0.3782\n",
      "Epoch 17/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.6928 - accuracy: 0.3850 - val_loss: 1.7255 - val_accuracy: 0.3692\n",
      "Epoch 18/100\n",
      "45/45 [==============================] - 1s 20ms/step - loss: 1.6863 - accuracy: 0.3906 - val_loss: 1.7807 - val_accuracy: 0.3570\n",
      "Epoch 19/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.6706 - accuracy: 0.3957 - val_loss: 1.6528 - val_accuracy: 0.3964\n",
      "Epoch 20/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.6793 - accuracy: 0.3902 - val_loss: 1.7164 - val_accuracy: 0.3756\n",
      "Epoch 21/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.6468 - accuracy: 0.4080 - val_loss: 1.6939 - val_accuracy: 0.3818\n",
      "Epoch 22/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.6228 - accuracy: 0.4109 - val_loss: 1.6731 - val_accuracy: 0.3932\n",
      "Epoch 23/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.6199 - accuracy: 0.4194 - val_loss: 1.6637 - val_accuracy: 0.3946\n",
      "Epoch 24/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.5975 - accuracy: 0.4271 - val_loss: 1.6836 - val_accuracy: 0.3924\n",
      "Epoch 25/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.5969 - accuracy: 0.4237 - val_loss: 1.7383 - val_accuracy: 0.3822\n",
      "Epoch 26/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.6183 - accuracy: 0.4161 - val_loss: 1.7654 - val_accuracy: 0.3594\n",
      "Epoch 27/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.5873 - accuracy: 0.4270 - val_loss: 1.6384 - val_accuracy: 0.3980\n",
      "Epoch 28/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.5809 - accuracy: 0.4280 - val_loss: 1.6362 - val_accuracy: 0.4116\n",
      "Epoch 29/100\n",
      "45/45 [==============================] - 1s 20ms/step - loss: 1.5699 - accuracy: 0.4338 - val_loss: 1.6000 - val_accuracy: 0.4258\n",
      "Epoch 30/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.5378 - accuracy: 0.4468 - val_loss: 1.5821 - val_accuracy: 0.4284\n",
      "Epoch 31/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.5423 - accuracy: 0.4421 - val_loss: 1.7200 - val_accuracy: 0.3726\n",
      "Epoch 32/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.5233 - accuracy: 0.4539 - val_loss: 1.6398 - val_accuracy: 0.4046\n",
      "Epoch 33/100\n",
      "45/45 [==============================] - 1s 20ms/step - loss: 1.5078 - accuracy: 0.4576 - val_loss: 1.5722 - val_accuracy: 0.4366\n",
      "Epoch 34/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.5047 - accuracy: 0.4565 - val_loss: 1.6653 - val_accuracy: 0.4044\n",
      "Epoch 35/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.5239 - accuracy: 0.4538 - val_loss: 1.6476 - val_accuracy: 0.4044\n",
      "Epoch 36/100\n",
      "45/45 [==============================] - 1s 18ms/step - loss: 1.5096 - accuracy: 0.4547 - val_loss: 1.5950 - val_accuracy: 0.4344\n",
      "Epoch 37/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.4918 - accuracy: 0.4662 - val_loss: 1.5567 - val_accuracy: 0.4406\n",
      "Epoch 38/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.4749 - accuracy: 0.4729 - val_loss: 1.5931 - val_accuracy: 0.4208\n",
      "Epoch 39/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.4539 - accuracy: 0.4732 - val_loss: 1.6480 - val_accuracy: 0.4230\n",
      "Epoch 40/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.4438 - accuracy: 0.4787 - val_loss: 1.5974 - val_accuracy: 0.4274\n",
      "Epoch 41/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.4355 - accuracy: 0.4810 - val_loss: 1.6308 - val_accuracy: 0.4236\n",
      "Epoch 42/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.4380 - accuracy: 0.4824 - val_loss: 1.7167 - val_accuracy: 0.4058\n",
      "Epoch 43/100\n",
      "45/45 [==============================] - 1s 18ms/step - loss: 1.4452 - accuracy: 0.4827 - val_loss: 1.6034 - val_accuracy: 0.4342\n",
      "Epoch 44/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.4110 - accuracy: 0.4953 - val_loss: 1.6140 - val_accuracy: 0.4218\n",
      "Epoch 45/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.3976 - accuracy: 0.4959 - val_loss: 1.5890 - val_accuracy: 0.4254\n",
      "Epoch 46/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.4011 - accuracy: 0.4977 - val_loss: 1.5973 - val_accuracy: 0.4360\n",
      "Epoch 47/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.4012 - accuracy: 0.4949 - val_loss: 1.5989 - val_accuracy: 0.4286\n",
      "Epoch 48/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.4068 - accuracy: 0.4910 - val_loss: 1.6167 - val_accuracy: 0.4298\n",
      "Epoch 49/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.4104 - accuracy: 0.4921 - val_loss: 1.5668 - val_accuracy: 0.4438\n",
      "Epoch 50/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.3718 - accuracy: 0.5066 - val_loss: 1.5818 - val_accuracy: 0.4442\n",
      "Epoch 51/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.3661 - accuracy: 0.5056 - val_loss: 1.5448 - val_accuracy: 0.4570\n",
      "Epoch 52/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.3539 - accuracy: 0.5112 - val_loss: 1.5835 - val_accuracy: 0.4426\n",
      "Epoch 53/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.3524 - accuracy: 0.5153 - val_loss: 1.5779 - val_accuracy: 0.4388\n",
      "Epoch 54/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.3333 - accuracy: 0.5192 - val_loss: 1.5920 - val_accuracy: 0.4398\n",
      "Epoch 55/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.3522 - accuracy: 0.5146 - val_loss: 1.6370 - val_accuracy: 0.4400\n",
      "Epoch 56/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.3280 - accuracy: 0.5255 - val_loss: 1.5707 - val_accuracy: 0.4616\n",
      "Epoch 57/100\n",
      "45/45 [==============================] - 1s 20ms/step - loss: 1.3231 - accuracy: 0.5195 - val_loss: 1.5681 - val_accuracy: 0.4586\n",
      "Epoch 58/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.2977 - accuracy: 0.5297 - val_loss: 1.5762 - val_accuracy: 0.4538\n",
      "Epoch 59/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.2916 - accuracy: 0.5345 - val_loss: 1.6017 - val_accuracy: 0.4430\n",
      "Epoch 60/100\n",
      "45/45 [==============================] - 1s 20ms/step - loss: 1.2861 - accuracy: 0.5351 - val_loss: 1.6756 - val_accuracy: 0.4314\n",
      "Epoch 61/100\n",
      "45/45 [==============================] - 1s 20ms/step - loss: 1.2827 - accuracy: 0.5385 - val_loss: 1.5920 - val_accuracy: 0.4512\n",
      "Epoch 62/100\n",
      "45/45 [==============================] - 1s 18ms/step - loss: 1.2692 - accuracy: 0.5387 - val_loss: 1.5937 - val_accuracy: 0.4514\n",
      "Epoch 63/100\n",
      "45/45 [==============================] - 1s 18ms/step - loss: 1.2628 - accuracy: 0.5435 - val_loss: 1.6038 - val_accuracy: 0.4484\n",
      "Epoch 64/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.2544 - accuracy: 0.5459 - val_loss: 1.5637 - val_accuracy: 0.4634\n",
      "Epoch 65/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.2420 - accuracy: 0.5554 - val_loss: 1.6112 - val_accuracy: 0.4422\n",
      "Epoch 66/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.2303 - accuracy: 0.5553 - val_loss: 1.6340 - val_accuracy: 0.4306\n",
      "Epoch 67/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.2624 - accuracy: 0.5458 - val_loss: 1.6118 - val_accuracy: 0.4490\n",
      "Epoch 68/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.2150 - accuracy: 0.5610 - val_loss: 1.6090 - val_accuracy: 0.4482\n",
      "Epoch 69/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.1894 - accuracy: 0.5716 - val_loss: 1.6961 - val_accuracy: 0.4226\n",
      "Epoch 70/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.2180 - accuracy: 0.5630 - val_loss: 1.6468 - val_accuracy: 0.4380\n",
      "Epoch 71/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.2159 - accuracy: 0.5623 - val_loss: 1.6537 - val_accuracy: 0.4402\n",
      "157/157 [==============================] - 1s 3ms/step - loss: 1.5448 - accuracy: 0.4570\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.544802188873291, 0.4569999873638153]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(Dense(100, activation='selu', kernel_initializer='lecun_uniform'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", \n",
    "              optimizer=tf.keras.optimizers.Nadam(lr=5e-4),\n",
    "              metrics='accuracy')\n",
    "\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=20)\n",
    "model_checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "    'modules/my_cifar10_selu_model.h5', save_best_only=True)\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb]\n",
    "\n",
    "X_means = X_train.mean(axis=0)\n",
    "X_stds = X_train.std(axis=0)\n",
    "X_train_scaled = (X_train - X_means) / X_stds\n",
    "X_valid_scaled = (X_valid - X_means) / X_stds\n",
    "X_test_scaled = (X_test - X_means) / X_stds\n",
    "\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=1000,\n",
    "          validation_data=(X_valid, y_valid), validation_batch_size=1000,\n",
    "          callbacks=callbacks)\n",
    "\n",
    "model = tf.keras.models.load_model(\"modules/my_cifar10_selu_model.h5\")\n",
    "model.evaluate(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We get 46% accuracy, which is not much better than the original model (40%), as good as the model using batch normalization (45%). However, convergence was almost as fast as with the BN model, plus each epoch took only 1 seconds. So it's by far the fastest model to train so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e. Exercise: Try regularizing the model with alpha dropout. Then, without retraining your model, see if you can achieve better accuracy using MC Dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "45/45 [==============================] - 5s 29ms/step - loss: 2.8423 - accuracy: 0.1153 - val_loss: 2.4905 - val_accuracy: 0.0990\n",
      "Epoch 2/100\n",
      "45/45 [==============================] - 1s 20ms/step - loss: 2.3019 - accuracy: 0.1626 - val_loss: 2.1629 - val_accuracy: 0.1836\n",
      "Epoch 3/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 2.1741 - accuracy: 0.2022 - val_loss: 2.0297 - val_accuracy: 0.2834\n",
      "Epoch 4/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 2.0488 - accuracy: 0.2516 - val_loss: 1.9087 - val_accuracy: 0.2972\n",
      "Epoch 5/100\n",
      "45/45 [==============================] - 1s 18ms/step - loss: 1.9812 - accuracy: 0.2747 - val_loss: 1.9187 - val_accuracy: 0.3016\n",
      "Epoch 6/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.9263 - accuracy: 0.3048 - val_loss: 1.9809 - val_accuracy: 0.2588\n",
      "Epoch 7/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.9141 - accuracy: 0.3017 - val_loss: 1.9069 - val_accuracy: 0.3222\n",
      "Epoch 8/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.8861 - accuracy: 0.3141 - val_loss: 1.8685 - val_accuracy: 0.3238\n",
      "Epoch 9/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.8430 - accuracy: 0.3354 - val_loss: 1.8082 - val_accuracy: 0.3364\n",
      "Epoch 10/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.8420 - accuracy: 0.3342 - val_loss: 1.8194 - val_accuracy: 0.3392\n",
      "Epoch 11/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.8001 - accuracy: 0.3488 - val_loss: 1.7755 - val_accuracy: 0.3672\n",
      "Epoch 12/100\n",
      "45/45 [==============================] - 1s 18ms/step - loss: 1.7702 - accuracy: 0.3592 - val_loss: 1.7890 - val_accuracy: 0.3630\n",
      "Epoch 13/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.7713 - accuracy: 0.3572 - val_loss: 1.7350 - val_accuracy: 0.3796\n",
      "Epoch 14/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.7607 - accuracy: 0.3669 - val_loss: 1.6933 - val_accuracy: 0.3972\n",
      "Epoch 15/100\n",
      "45/45 [==============================] - 1s 18ms/step - loss: 1.7162 - accuracy: 0.3791 - val_loss: 1.6852 - val_accuracy: 0.3994\n",
      "Epoch 16/100\n",
      "45/45 [==============================] - 1s 18ms/step - loss: 1.7258 - accuracy: 0.3817 - val_loss: 1.8391 - val_accuracy: 0.3484\n",
      "Epoch 17/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.6984 - accuracy: 0.3865 - val_loss: 1.7240 - val_accuracy: 0.3876\n",
      "Epoch 18/100\n",
      "45/45 [==============================] - 1s 18ms/step - loss: 1.6814 - accuracy: 0.3934 - val_loss: 1.8750 - val_accuracy: 0.3366\n",
      "Epoch 19/100\n",
      "45/45 [==============================] - 1s 18ms/step - loss: 1.6989 - accuracy: 0.3942 - val_loss: 1.6449 - val_accuracy: 0.4186\n",
      "Epoch 20/100\n",
      "45/45 [==============================] - 1s 18ms/step - loss: 1.6349 - accuracy: 0.4147 - val_loss: 1.7150 - val_accuracy: 0.3860\n",
      "Epoch 21/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.6336 - accuracy: 0.4132 - val_loss: 1.7029 - val_accuracy: 0.4044\n",
      "Epoch 22/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.6291 - accuracy: 0.4175 - val_loss: 1.6918 - val_accuracy: 0.4052\n",
      "Epoch 23/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.6126 - accuracy: 0.4237 - val_loss: 1.6769 - val_accuracy: 0.4142\n",
      "Epoch 24/100\n",
      "45/45 [==============================] - 1s 18ms/step - loss: 1.6053 - accuracy: 0.4248 - val_loss: 1.7050 - val_accuracy: 0.4036\n",
      "Epoch 25/100\n",
      "45/45 [==============================] - 1s 17ms/step - loss: 1.5990 - accuracy: 0.4308 - val_loss: 1.7187 - val_accuracy: 0.4014\n",
      "Epoch 26/100\n",
      "45/45 [==============================] - 1s 18ms/step - loss: 1.5851 - accuracy: 0.4303 - val_loss: 1.6434 - val_accuracy: 0.4216\n",
      "Epoch 27/100\n",
      "45/45 [==============================] - 1s 18ms/step - loss: 1.5771 - accuracy: 0.4363 - val_loss: 1.6569 - val_accuracy: 0.4204\n",
      "Epoch 28/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.5637 - accuracy: 0.4364 - val_loss: 1.6187 - val_accuracy: 0.4278\n",
      "Epoch 29/100\n",
      "45/45 [==============================] - 1s 18ms/step - loss: 1.5675 - accuracy: 0.4408 - val_loss: 1.6072 - val_accuracy: 0.4348\n",
      "Epoch 30/100\n",
      "45/45 [==============================] - 1s 18ms/step - loss: 1.5381 - accuracy: 0.4503 - val_loss: 1.6062 - val_accuracy: 0.4338\n",
      "Epoch 31/100\n",
      "45/45 [==============================] - 1s 18ms/step - loss: 1.5391 - accuracy: 0.4497 - val_loss: 1.6047 - val_accuracy: 0.4462\n",
      "Epoch 32/100\n",
      "45/45 [==============================] - 1s 17ms/step - loss: 1.5158 - accuracy: 0.4569 - val_loss: 1.6015 - val_accuracy: 0.4308\n",
      "Epoch 33/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.5216 - accuracy: 0.4577 - val_loss: 1.6641 - val_accuracy: 0.4156\n",
      "Epoch 34/100\n",
      "45/45 [==============================] - 1s 18ms/step - loss: 1.5270 - accuracy: 0.4559 - val_loss: 1.8685 - val_accuracy: 0.3714\n",
      "Epoch 35/100\n",
      "45/45 [==============================] - 1s 18ms/step - loss: 1.5250 - accuracy: 0.4583 - val_loss: 1.6891 - val_accuracy: 0.4244\n",
      "Epoch 36/100\n",
      "45/45 [==============================] - 1s 18ms/step - loss: 1.5092 - accuracy: 0.4611 - val_loss: 1.6248 - val_accuracy: 0.4438\n",
      "Epoch 37/100\n",
      "45/45 [==============================] - 1s 18ms/step - loss: 1.4683 - accuracy: 0.4763 - val_loss: 1.5785 - val_accuracy: 0.4514\n",
      "Epoch 38/100\n",
      "45/45 [==============================] - 1s 18ms/step - loss: 1.4593 - accuracy: 0.4796 - val_loss: 1.6809 - val_accuracy: 0.4274\n",
      "Epoch 39/100\n",
      "45/45 [==============================] - 1s 18ms/step - loss: 1.4668 - accuracy: 0.4726 - val_loss: 1.5727 - val_accuracy: 0.4580\n",
      "Epoch 40/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.4497 - accuracy: 0.4796 - val_loss: 1.6646 - val_accuracy: 0.4176\n",
      "Epoch 41/100\n",
      "45/45 [==============================] - 1s 18ms/step - loss: 1.4509 - accuracy: 0.4796 - val_loss: 1.6062 - val_accuracy: 0.4472\n",
      "Epoch 42/100\n",
      "45/45 [==============================] - 1s 18ms/step - loss: 1.4435 - accuracy: 0.4828 - val_loss: 1.6001 - val_accuracy: 0.4552\n",
      "Epoch 43/100\n",
      "45/45 [==============================] - 1s 18ms/step - loss: 1.4391 - accuracy: 0.4882 - val_loss: 1.7267 - val_accuracy: 0.4198\n",
      "Epoch 44/100\n",
      "45/45 [==============================] - 1s 18ms/step - loss: 1.4407 - accuracy: 0.4862 - val_loss: 1.5736 - val_accuracy: 0.4608\n",
      "Epoch 45/100\n",
      "45/45 [==============================] - 1s 18ms/step - loss: 1.4104 - accuracy: 0.4926 - val_loss: 1.6060 - val_accuracy: 0.4366\n",
      "Epoch 46/100\n",
      "45/45 [==============================] - 2s 39ms/step - loss: 1.4044 - accuracy: 0.4968 - val_loss: 1.6937 - val_accuracy: 0.4422\n",
      "Epoch 47/100\n",
      "45/45 [==============================] - 1s 17ms/step - loss: 1.4146 - accuracy: 0.4929 - val_loss: 1.5757 - val_accuracy: 0.4504\n",
      "Epoch 48/100\n",
      "45/45 [==============================] - 1s 18ms/step - loss: 1.3919 - accuracy: 0.5005 - val_loss: 1.6429 - val_accuracy: 0.4450\n",
      "Epoch 49/100\n",
      "45/45 [==============================] - 1s 18ms/step - loss: 1.4267 - accuracy: 0.4861 - val_loss: 1.6369 - val_accuracy: 0.4506\n",
      "Epoch 50/100\n",
      "45/45 [==============================] - 1s 18ms/step - loss: 1.3811 - accuracy: 0.5071 - val_loss: 1.6057 - val_accuracy: 0.4538\n",
      "Epoch 51/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.3761 - accuracy: 0.5064 - val_loss: 1.7097 - val_accuracy: 0.4236\n",
      "Epoch 52/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.3888 - accuracy: 0.5035 - val_loss: 1.5916 - val_accuracy: 0.4596\n",
      "Epoch 53/100\n",
      "45/45 [==============================] - 1s 18ms/step - loss: 1.3564 - accuracy: 0.5133 - val_loss: 1.5827 - val_accuracy: 0.4668\n",
      "Epoch 54/100\n",
      "45/45 [==============================] - 1s 18ms/step - loss: 1.3586 - accuracy: 0.5119 - val_loss: 1.5964 - val_accuracy: 0.4510\n",
      "Epoch 55/100\n",
      "45/45 [==============================] - 1s 18ms/step - loss: 1.3344 - accuracy: 0.5213 - val_loss: 1.6184 - val_accuracy: 0.4622\n",
      "Epoch 56/100\n",
      "45/45 [==============================] - 1s 18ms/step - loss: 1.3424 - accuracy: 0.5160 - val_loss: 1.6106 - val_accuracy: 0.4588\n",
      "Epoch 57/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.3208 - accuracy: 0.5252 - val_loss: 1.6070 - val_accuracy: 0.4616\n",
      "Epoch 58/100\n",
      "45/45 [==============================] - 1s 19ms/step - loss: 1.3254 - accuracy: 0.5223 - val_loss: 1.6155 - val_accuracy: 0.4510\n",
      "Epoch 59/100\n",
      "45/45 [==============================] - 1s 18ms/step - loss: 1.3052 - accuracy: 0.5257 - val_loss: 1.6420 - val_accuracy: 0.4480\n",
      "157/157 [==============================] - 1s 3ms/step - loss: 1.5727 - accuracy: 0.4580\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.5727181434631348, 0.4580000042915344]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import AlphaDropout\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(Dense(100, activation='selu', kernel_initializer='lecun_uniform'))\n",
    "model.add(AlphaDropout(rate=0.1))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", \n",
    "              optimizer=tf.keras.optimizers.Nadam(lr=5e-4),\n",
    "              metrics='accuracy')\n",
    "\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=20)\n",
    "model_checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
    "    'modules/my_cifar10_alpha_dropout_model.h5', save_best_only=True)\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb]\n",
    "\n",
    "X_means = X_train.mean(axis=0)\n",
    "X_stds = X_train.std(axis=0)\n",
    "X_train_scaled = (X_train - X_means) / X_stds\n",
    "X_valid_scaled = (X_valid - X_means) / X_stds\n",
    "X_test_scaled = (X_test - X_means) / X_stds\n",
    "\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=1000,\n",
    "          validation_data=(X_valid, y_valid), validation_batch_size=1000,\n",
    "          callbacks=callbacks)\n",
    "\n",
    "model = tf.keras.models.load_model(\"modules/my_cifar10_alpha_dropout_model.h5\")\n",
    "model.evaluate(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Same accuracy as before"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f513c44cab0e7f43b93feb2c493bf956de6913ea868a31988c642fe6813eee3c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('dataScienceEnv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
