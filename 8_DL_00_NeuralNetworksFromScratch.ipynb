{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Neural Networks From Scratch\n",
    "Credits: [sentdex](https://www.youtube.com/playlist?list=PLQVvvaa0QuDcjD5BAw2DxE6OF2tius3V3), [book](https://nnfs.io/)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## A Signle Neuron "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "inputs  = [1,     2,    3, 2.5]\n",
    "weights = [0.2, 0.8, -0.5, 1.0]\n",
    "bias    = 2\n",
    "\n",
    "output = inputs[0]*weights[0] + inputs[1]*weights[1] + inputs[2]*weights[2] + inputs[3]*weights[3] + bias\n",
    "print(output)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "4.8\n"
     ]
    }
   ],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Multiple Neurons"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### pure python"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "inputs  = [     1,     2,    3,   2.5  ] # 1x4\n",
    "\n",
    "weights = [ [ 0.2,   0.8,  -0.5,  1.0],\n",
    "            [ 0.5,  -0.91, 0.26, -0.5],\n",
    "            [-0.26, -0.27, 0.17,  0.87] ] # 3x4\n",
    "\n",
    "biases  = [ 2, 3, 0.5 ] # 1x3\n",
    "\n",
    "layer_outputs = [] \n",
    "for neuron_weights, neuron_bias in zip(weights, biases):\n",
    "    neuron_output = 0\n",
    "    for n_input, weight in zip(inputs, neuron_weights):\n",
    "        neuron_output += n_input*weight\n",
    "    neuron_output += neuron_bias\n",
    "    layer_outputs.append(neuron_output)\n",
    "\n",
    "print(layer_outputs)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[4.8, 1.21, 2.385]\n"
     ]
    }
   ],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### numpy"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import numpy as np\n",
    "\n",
    "inputs  = [     1,     2,    3,   2.5  ]\n",
    "\n",
    "weights = [ [ 0.2,   0.8, -0.5,   1.0],\n",
    "            [ 0.5,  -0.91, 0.26, -0.5],\n",
    "            [-0.26, -0.27, 0.17,  0.87] ]\n",
    "\n",
    "biases  = [ 2, 3, 0.5 ]\n",
    "\n",
    "layer_outputs = np.dot(weights, inputs) + biases\n",
    "\n",
    "print(layer_outputs)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[4.8   1.21  2.385]\n"
     ]
    }
   ],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Batches"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# multiple inputs (batch)\n",
    "import numpy as np\n",
    "\n",
    "inputs  = [ [ 1,   2,   3,    2.5],\n",
    "            [ 2,   5,  -1,    2  ],\n",
    "            [-1.5, 2.7, 3.3, -0.8] ]\n",
    "\n",
    "weights = [ [ 0.2,   0.8, -0.5,   1.0],\n",
    "            [ 0.5,  -0.91, 0.26, -0.5],\n",
    "            [-0.26, -0.27, 0.17,  0.87] ]\n",
    "\n",
    "biases  = [ 2, 3, 0.5 ]\n",
    "\n",
    "layer_outputs = np.dot(inputs, np.array(weights).T) + biases\n",
    "# column wise outputs\n",
    "# layer_outputs = np.dot(weights, np.array(inputs).T) + np.reshape(biases, (3,1))\n",
    "\n",
    "print(layer_outputs)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[ 4.8    1.21   2.385]\n",
      " [ 8.9   -1.81   0.2  ]\n",
      " [ 1.41   1.051  0.026]]\n"
     ]
    }
   ],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Layers"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# multiple layers --> n_(l+1) x n_l\n",
    "import numpy as np\n",
    "\n",
    "inputs   = [ [ 1,   2,   3,    2.5],\n",
    "            [ 2,   5,  -1,    2  ],\n",
    "            [-1.5, 2.7, 3.3, -0.8] ] # m x n = 3 x 4 \n",
    "           \n",
    "weights  = [ [ 0.2,   0.8, -0.5,   1.0],\n",
    "            [ 0.5,  -0.91, 0.26, -0.5],\n",
    "            [-0.26, -0.27, 0.17,  0.87] ] # n_l1 x n = 3 x 4\n",
    "\n",
    "biases   = [ 2, 3, 0.5 ] # n_l1 = 3\n",
    "\n",
    "weights2 = [ [ 0.1, -0.14, 0.5  ],\n",
    "            [ 0.5,   0.12, -0.33] ] # n_l2 x n_l1 = 2 x 3\n",
    "\n",
    "biases2  = [ -1, 2 ] # n_l2 = 2\n",
    "\n",
    "layer1_outputs = np.dot(inputs, np.array(weights).T) + biases # m x n_l1 = 3 x 3\n",
    "layer2_outputs = np.dot(layer1_outputs, np.array(weights2).T) + biases2 # m x n_l2 = 3 x 2\n",
    "\n",
    "print(layer2_outputs)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[ 0.5031   3.75815]\n",
      " [ 0.2434   6.1668 ]\n",
      " [-0.99314  2.82254]]\n"
     ]
    }
   ],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## A Dense Layer Object"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# using objects for generality\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "X = [ [ 1,   2,   3,    2.5],\n",
    "      [ 2,   5,  -1,    2  ],\n",
    "      [-1.5, 2.7, 3.3, -0.8] ]\n",
    "           \n",
    "class Layer_Dense:\n",
    "    \n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = 0.1 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases  = np.zeros((1, n_neurons))\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "    \n",
    "layer1 = Layer_Dense(4, 5)\n",
    "layer2 = Layer_Dense(5, 2)\n",
    "\n",
    "layer1.forward(X)\n",
    "print(layer1.output)\n",
    "\n",
    "layer2.forward(layer1.output)\n",
    "print(layer2.output)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[ 0.10758131  1.03983522  0.24462411  0.31821498  0.18851053]\n",
      " [-0.08349796  0.70846411  0.00293357  0.44701525  0.36360538]\n",
      " [-0.50763245  0.55688422  0.07987797 -0.34889573  0.04553042]]\n",
      "[[ 0.148296   -0.08397602]\n",
      " [ 0.14100315 -0.01340469]\n",
      " [ 0.20124979 -0.07290616]]\n"
     ]
    }
   ],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Activation Functions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "!pip install nnfs"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: nnfs in /nfs/home5/p285464/.conda/envs/torchEnv/lib/python3.7/site-packages (0.5.1)\n",
      "Requirement already satisfied: numpy in /nfs/home5/p285464/.conda/envs/torchEnv/lib/python3.7/site-packages (from nnfs) (1.20.1)\n"
     ]
    }
   ],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# using ReLU activation functions\n",
    "\n",
    "import numpy as np\n",
    "import nnfs # for data set\n",
    "\n",
    "nnfs.init()\n",
    "from nnfs.datasets import spiral_data\n",
    "\n",
    "# 100 samples per class\n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "X.shape, y.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((300, 2), (300,))"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ReLU for hidden layers and Softmax for output layer\n",
    "- Avoid possible overflow as exponentiation may lead to large values:\n",
    "\n",
    "    $z_i = z_i - max(z)$ for each row $z$ in input\n",
    "\n",
    "    $=> Z_{i,j} \\le 0$\n",
    "\n",
    "    $=> e^{Z_{i,j}} \\in [0,1]$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = 0.1 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases  = np.zeros((1, n_neurons))\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "    \n",
    "class Activation_ReLU:\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "class Activation_Softmax:\n",
    "    def forward(self, inputs):\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        self.output = exp_values/np.sum(exp_values, axis=1, keepdims=True)\n",
    "        \n",
    "# Hidden Layer\n",
    "denseLayer1 = Layer_Dense(2, 3)\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "denseLayer1.forward(X)\n",
    "activation1.forward(denseLayer1.output)\n",
    "\n",
    "# Output Layer\n",
    "denseLayer2 = Layer_Dense(3, 3)\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "denseLayer2.forward(activation1.output)\n",
    "activation2.forward(denseLayer2.output)\n",
    "\n",
    "# Probability for each sample (300) for each class (3)\n",
    "print(activation2.output.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(300, 3)\n"
     ]
    }
   ],
   "metadata": {
    "collapsed": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loss with Categorical Cross-Entropy\n",
    "\n",
    "- One way to avoid zeros in log for the cross-entropy log function is to clip the outputs (y_preds).\n",
    "- Handling on-hot or not-one-hot encoded y_ture values\n",
    "    - No-one-hot: [1,0,1,2]: use numpy fancy slicing used to get the probabilities (from y_pred) for which y_true is 1 (in the one-hot encoded vector) for each sample.\n",
    "    - One-hot: [[0,1,0,0],[1,0,0,0],[0,1,0,0],[0,0,0,1]]: Use element-wise multiplicaiton and then sum along columns."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "# Dense Layer\n",
    "class Layer_Dense:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = 0.1 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases  = np.zeros((1, n_neurons))\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "# Activation Functions    \n",
    "class Activation_ReLU:\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "class Activation_Softmax:\n",
    "    def forward(self, inputs):\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        self.output = exp_values/np.sum(exp_values, axis=1, keepdims=True)\n",
    "\n",
    "# Loss\n",
    "class Loss:\n",
    "    def calculate(self, output, y):\n",
    "        sample_losses = self.forward(output, y)\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        return data_loss\n",
    "\n",
    "class Loss_CategoricalCrossEntropy(Loss):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        samples = len(y_pred)\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7)\n",
    "        if len(y_true.shape)==1:\n",
    "            correct_confidences = y_pred_clipped[range(samples), y_true]\n",
    "        elif len(y_true.shape)==1:\n",
    "            correct_confidences = np.sum(y_pred_clipped*y_true, axis=1)\n",
    "        neg_log_likelihoods = -np.log(correct_confidences)\n",
    "        return neg_log_likelihoods\n",
    "\n",
    "# Hidden Layer\n",
    "denseLayer1 = Layer_Dense(2, 3)\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "denseLayer1.forward(X)\n",
    "activation1.forward(denseLayer1.output)\n",
    "\n",
    "# Output Layer\n",
    "denseLayer2 = Layer_Dense(3, 3)\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "denseLayer2.forward(activation1.output)\n",
    "activation2.forward(denseLayer2.output)\n",
    "\n",
    "# Probability for each sample (300) for each class (3)\n",
    "print(activation2.output.shape)\n",
    "\n",
    "# Loss\n",
    "loss_function = Loss_CategoricalCrossEntropy()\n",
    "loss = loss_function.calculate(activation2.output, y)\n",
    "print(loss)\n",
    "\n",
    "# Accuracy\n",
    "accuracy = np.mean(np.argmax(activation2.output, axis=1) == y)*100\n",
    "print(f\"Accuracy = {accuracy}\")\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(300, 3)\n",
      "1.0983309\n",
      "Accuracy = 33.666666666666664\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('torchEnv': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "interpreter": {
   "hash": "c3b1f0aa73ce6a53838a2507dee39748d09bb5596497487604e75b434ade7b74"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}