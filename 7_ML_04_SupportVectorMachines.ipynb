{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "663f9274",
   "metadata": {},
   "source": [
    "# ML_SupportVectorMachines\n",
    "\n",
    "Credits:\n",
    "\n",
    "- [2021 Python for Machine Learning & Data Science Masterclass by Jose Portilla Udemy](https://www.udemy.com/course/python-for-machine-learning-data-science-masterclass/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e21ab69",
   "metadata": {},
   "source": [
    "Inventor: Vladimir Vapnik\n",
    "\n",
    "Support Vector Machines are one of the more complex algorithms we will learn, but it all begins with a simple premise: Does a *hyperplane* exist that can effectively separate classes?\n",
    "\n",
    "- **Hyperplane**: In an N-dimensional space, a hyperplane is a flat affine subspace of hyperplane dimension Nâˆ’1. For example:\n",
    "    - 1-D Hyperplane is a single point\n",
    "    - 2-D Hyperplane is a line\n",
    "    - 3-D Hyperplane is flat plane\n",
    "\n",
    "We mostly use the term hyperplance when we can't draw it on paper. The main idea behind SVM is that we can use Hyperplanes to create a separation between classes. \n",
    "\n",
    "\n",
    "- **Maximum Margin Classifier**\n",
    "<img src='images/SVM1.png' width='800'>\n",
    "\n",
    "\n",
    "- **Support Vector Classifier**: the name comes from the fact that the observations on the edges and within the Soft Margin are called Support Vectors.\n",
    "<img src='images/SVM2.png' width='800'>\n",
    "\n",
    "\n",
    "- **Support Vector Machines**: use Kernel Functions to systematically find Support Vector Classifiers in higher dimensions.\n",
    "<img src='images/SVM3.png' width='800'>\n",
    "\n",
    "- Kernel functions only calculate the relationships b/w every pair points as if they are in the higher dimensions. They don't actually do the transformation. Thus, they reduce the amount of computation required for SVM by avoiding the transformations from low to high dimensions.\n",
    "\n",
    "<img src='images/SVM5.png' width='800'>\n",
    "\n",
    "- From the figure above, the purpose of the Max Margin Classifier is to maximize M (which is the margin extending from the hyperplane : dist b/w the dotted and the solid line) by choosing the best $\\beta$ coeffs. The contraints are that \n",
    "    - no data points can be within the margin: $y_{i} (\\beta_0+\\beta_1x_{i1}+....)\\ge M$ \n",
    "    - the orthogonal distance from any data point to the hyperplane is greater than the margin: $\\sum_{j=1}^{p} \\beta_j^2=1$\n",
    "\n",
    "- A Support Vector Classifier is same as the Max Margin Classifier, except that it allows for some misclassificatoin vis a $C$ value.\n",
    "    - Sum of the error (wrt to margin): $\\sum_{i=1}^n\\epsilon_i \\le C$\n",
    "    - the margin with allowed errors: $y_{i} (\\beta_0+\\beta_1x_{i1}+....)\\ge M(1-\\epsilon_i)$ \n",
    "\n",
    "\n",
    "*NOTE: in scikit-learn the $C$ value is inversly proprotional to the $C$ value in the equaitons above!*\n",
    "\n",
    "**Connection b/w Kernel and Dot-product**\n",
    "- Polynomial Kernel\n",
    "\n",
    "<img src='images/SVM6.png' width='800'>\n",
    "\n",
    "\n",
    "- Radial Basis Function (RBF) / Radial Kernel\n",
    "\n",
    "<img src='images/SVM7.png' width='800'>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fddf16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
